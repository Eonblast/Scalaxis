% Copyright 2007-2010 Konrad-Zuse-Zentrum für Informationstechnik Berlin

% Licensed under the Apache License, Version 2.0 (the "License");
% you may not use this file except in compliance with the License.
% You may obtain a copy of the License at
%
%     http://www.apache.org/licenses/LICENSE-2.0
%
% Unless required by applicable law or agreed to in writing, software
% distributed under the License is distributed on an "AS IS" BASIS,
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
% See the License for the specific language governing permissions and
% limitations under the License.
\documentclass[a4]{scrreprt}
\usepackage{typearea}
\areaset[1cm]{165mm}{240mm}

\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc}

\usepackage{relsize}
\usepackage{graphicx}
%\usepackage{color}
%\usepackage{colortbl}
\usepackage{longtable}
\usepackage{makeidx}
\usepackage{ifthen}
\usepackage{fancyhdr}
\usepackage{fancyvrb}
\usepackage{lastpage}
\usepackage{xcolor}
\usepackage[pdftex,
        colorlinks=true,
        urlcolor=rltblue,       % \href{...}{...} external (URL)
        filecolor=rltblue,     % \href{...} local file
        linkcolor=rltblue,      % \ref{...} and \pageref{...}
%        pagebackref=true,
        pdfborder={0 0 0}]{hyperref}
\usepackage{listings}
\usepackage{etextools}

\renewcommand{\headrulewidth}{0pt}    % Width of head rule
\renewcommand{\footrulewidth}{0.3pt}  % Width of head rule

% normal pages
\pagestyle{fancy}
\fancyhf{} % clear all header and footer fields
%\fancyhead[RE,LO]{}
\fancyhead[R]{}%
\fancyfoot[R]{\bfseries\thepage\ / \pageref{LastPage}}%
\chead{}%
\cfoot{}%

% beginning of a chapter
\fancypagestyle{plain}{%
\fancyhf{} % clear all header and footer fields
%\fancyhead[RE,LO]{}
\fancyhead[R]{}%
\fancyfoot[R]{\bfseries\thepage\ / \pageref{LastPage}}%
\chead{}%
\cfoot{}%
}

% Clear Header Style on the Last Empty Odd pages
\makeatletter
\def\cleardoublepage{\clearpage\if@twoside \ifodd\c@page\else%
    \hbox{}%
    \thispagestyle{empty}%              % Empty header styles
    \newpage%
    \if@twocolumn\hbox{}\newpage\fi\fi\fi}
\makeatother

%% Bold typewriter font
%\renewcommand{\ttdefault}{pcr}
%\renewcommand{\rmdefault}{ptm} % Times
%\renewcommand{\rmdefault}{ppl} % Palatino
%%\renewcommand{\rmdefault}{pnc} % NewCenturySchoolbook
%%\renewcommand{\rmdefault}{pbk} % BookMan
%\renewcommand{\rmdefault}{pag} % Avantgarde (sans serif)
%% \renewcommand{\rmdefault}{pzc} % ZapfChancery (slanted)
%\renewcommand{\rmdefault}{put} % Utopia
\renewcommand{\rmdefault}{bch} % CharterBT


%%% colors %%%%%%%%%%%%%%%%%%%%%%%%

\definecolor{lightyellow}{rgb}{1.0, 1.0, 0.5}
\definecolor{rltred}{rgb}{0.75,0,0}
\definecolor{rltgreen}{rgb}{0,0.5,0}


%\definecolor{rltblue}{rgb}{0,0,0.75}
\definecolor{rltblue}{HTML}{1F4980}
\definecolor{lightgray}{gray}{0.9}

\setlength{\parindent}{0pt}

% 31 73 128 blue

\definecolor{lightyellow}{rgb}{1.0, 1.0, 0.5}
\definecolor{codebackground}{rgb}{0.9,0.95,1.0}
\definecolor{commandinput}{rgb}{0.8,0.8,1}

%\thicklines
\lstset{
  basicstyle=\scriptsize\ttfamily,
  backgroundcolor=\color{codebackground},
  keywordstyle=\color{blue}\bfseries,
  % underlined bold black keywords
  identifierstyle=\bfseries, % nothing happens
  commentstyle=\color{red}\bfseries, % white comments
  stringstyle=\sffamily, % typewriter type for strings
  showstringspaces=false,
  xleftmargin=3pt,
  xrightmargin=3pt,
  fancyvrb=true,
  frame=single,
%  frameround=tttt,
%  framexleftmargin=0pt,
  framextopmargin=3pt,
  framexbottommargin=3pt,
%  framexrightmargin=5pt,
  rulecolor=\color{codebackground},
  language=erlang,
%  fillcolor=\color{red},
%  rulesepcolor=\color{black}
%  rulesep=1cm,
}
\lstset{rangebeginprefix=\%\%\ userdevguide-begin\ }
\lstset{rangeendprefix=\%\%\ userdevguide-end\ }
\lstset{includerangemarker=false}

% \codesnippet[lstsettings]{filename-label}{range-label}{src-file with path}
% \codesnippet[language=erlang]{admin.erl}{admin:add_nodes}{../src/admin.erl}
\newcommand{\codesnippet}[4][language=erlang]{
{%% File: \url{#4}
\lstset{numbers=left}
\lstinputlisting[
  title=\filetitle{#2},
  linerange=#3-#3,
  #1
]
{#4}
}
}

% \codefile[lstsettings]{filename-label}{src-file with path}
% \codefile[language=erlang]{admin.erl}{../src/admin.erl}
\newcommand{\codefile}[3][language=erlang]{
{
\lstinputlisting[
  title=\filetitle{#2},
  #1]
{#3}
}
}
\newcommand{\bfref}[1]{\textbf{\hyperpage{#1}}}
\newcommand{\sieheref}[1]{\ref{#1} on page~\pageref{#1}}
\newcommand{\code}[1]{\lstinline[basicstyle=\small\ttfamily]!#1!}
\newcommand{\filetitle}[1]{\hbox to \linewidth{~~File \code{#1:}\hfill}}
\newcommand{\scalaris}{Scalaris}
\newcommand{\todo}[1]{{\color{red}{\em TODO: #1}}}
\newcommand{\svnrev}[1]
{\hfill\emph{Description is based on SVN revision #1.}\medskip}

\newcommand{\erlparamsorarity}[1]{%
\ifthenelse{\equal{0}{\gettokslistindex{/}{#1}}}{\texttt{#1}}{(\texttt{#1})}}
\newcommand{\erlfun}[4][index]{%
\ifthenelse{\equal{index}{#1}}%
{\index{#2@\texttt{#2}!#3@\texttt{#3}}}%
{}%
\texttt{#2}:\-\texttt{#3}\texttt{\erlparamsorarity{#4}}}

\newcommand{\erlfunindex}[2]{%
\index{#1@\texttt{#1}!#2@\texttt{#2}|bfref}}%

\newcommand{\erlmodule}[2][index]{%
\ifthenelse{\equal{index}{#1}}%
{\index{#2@\texttt{#2}}}{}%
\texttt{#2}}

\newcommand{\erlmoduleindex}[1]{%
\index{#1@\texttt{#1}|bfref}}%

\makeatletter
\newenvironment{erlfunparams}
{
\list{}{
    \labelwidth\z@ \makeatother \itemindent-\leftmargin
    \let\makelabel\descriptionlabel
\setlength\leftmargin{4em}
\setlength{\parskip}{0pt}
\setlength{\parsep}{0pt}
\setlength{\itemsep}{0pt}
}}
{\endlist}
\makeatother
\newcommand{\erlparam}[2]{\item[\texttt{#1}:] #2}

\newenvironment{wikitext}
{\catcode`\#=13\catcode`\$=13\catcode`\_=11\catcode`\==13
}{\catcode`\#=6\catcode`\$=3\catcode`\_=8}


\makeindex

\begin{document}
\vspace*{4cm}
\thispagestyle{empty}
\setlength{\parskip}{1ex}
\begin{tabular}{p{4cm}p{0.5cm}p{10cm}}
%\vspace*{-5cm}\includegraphics[width=5cm]{scalaris-layers}
\parbox{4cm}{\includegraphics[width=4cm]{scalaris-layers}}
& &\sffamily\bfseries\Huge
  \bigskip {\textcolor{rltblue}{Scalaris:}}

\medskip
 \mdseries Users and Developers Guide

\bigskip\medskip
\LARGE Version 0.2.0 draft \hfill \today\\
\end{tabular}
\vfill
{\scriptsize
Copyright 2007-2010 Konrad-Zuse-Zentrum für Informationstechnik Berlin
and onScale solutions.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

\url{http://www.apache.org/licenses/LICENSE-2.0}

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
}

\tableofcontents

\part{Users Guide}

\chapter{Introduction}

Scalaris is a scalable, transactional, distributed key-value store based on
the peer-to-peer principle. It can be used to build scalable Web 2.0
services. The concept of Scalaris is quite simple: Its architecture consists
of three layers.

It provides self-management and scalability by replicating services and data
among peers. Without system interruption it scales from a few PCs to
thousands of servers. Servers can be added or removed on the fly without any
service downtime.

\begin{center}
\includegraphics[width=0.7\linewidth]{layers}
\end{center}

Scalaris takes care of:

\begin{itemize}
\item Fail-over
\item Data distribution
\item Replication
\item Strong consistency
\item Transactions
\end{itemize}

The Scalaris project was initiated by Zuse Institute Berlin and onScale
solutions and was partly funded by the EU projects Selfman and
XtreemOS. Additional information (papers, videos) can be found at
\url{http://www.zib.de/CSR/Projects/scalaris} and
\url{http://www.onscale.de/scalarix.html}.

\section{Brewer's CAP Theorem}

In distributed computing there exists the so called CAP theorem. It
basically says that there are three desirable properties for distributed
systems but one can only have any two of them.

\begin{description}
\item {Strict Consistency.} Any read operation has to return the
  result of the latest write operation on the same data item.

\item {Availability.} Items can be read and modified at any time.

\item {Partition Tolerance.} The network on which the service is
  running may split into several partitions which cannot communicate
  with each other. Later on the networks may re-join again.

  For example, a service is hosted on one machine in Seattle and one
  machine in Berlin. This service is partition tolerant if it can
  tolerate that all Internet connections over the Atlantic (and
  Pacific) are interrupted for a few hours and then get repaired.
\end{description}

The goal of \scalaris{} is to provide strict consistency and partition
tolerance. We are willing to sacrifice availability to make sure that
the stored data is always consistent. I.e. when you are running
\scalaris{} with a replication degree of 4 and the network splits into
two partitions, one partition with three replicas and one partition
with one replica, you will be able to continue to use the service only
in the larger partition. All requests in the smaller partition will
time out until the two networks merge again. Note, most other
key-value stores tend to sacrifice consistency.

\chapter{Download and Installation}

\section{Requirements}

For building and running \scalaris{}, some third-party modules are
required which are not included in the \scalaris{} sources:

\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
\item Erlang R13B01 or newer
\item GNU-like Make
\end{itemize}

To build the Java API (and the command-line client) the following
modules are required additionally:

\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
\item Java Development Kit 6
\item Apache Ant
\end{itemize}

Before building the Java API, make sure that \code{JAVA\_HOME} and
\code{ANT\_HOME} are set. \code{JAVA\_HOME} has to point to a JDK
installation, and \code{ANT\_HOME} has to point to an Ant installation.

\section{Download}

The sources can be obtained from
\url{http://code.google.com/p/scalaris}. RPMs and DEBs are available from
\url{http://download.opensuse.org/repositories/home:/tschuett/}.

\subsection{Development Branch}

You find the latest development version in the svn repository:
\begin{lstlisting}[language=sh]
# Non-members may check out a read-only working copy anonymously over HTTP.
svn checkout http://scalaris.googlecode.com/svn/trunk/ scalaris-read-only
\end{lstlisting}

\subsection{Releases}

Releases can be found under the 'Download' tab on the web-page.


\section{Configuration}

\scalaris{} reads two configuration files from the working directory:
\code{bin/scalaris.cfg} (mandatory) and \code{bin/scalaris.local.cfg}
(optional). The former defines default settings and is included in the
release. The latter can be created by the user to alter settings.  A
sample file is provided as \code{bin/scalaris.local.cfg.example}. To run
\scalaris{} distributed over several nodes, each node requires a
\code{bin/scalaris.local.cfg}:

\codefile{scalaris.local.cfg}{../bin/scalaris.local.cfg.example}

\scalaris{} currently distinguishes two different kinds of nodes: (a)
the boot-server and (b) regular nodes. For the moment, we limit the
number of boot-servers to exactly one. The remaining nodes are regular
nodes. The boot-server is contacted to join the system. On all servers,
the \code{boot_host} option defines the server where the boot server
is running. In the example, it is an IP address plus a TCP port.

\section{Build}

\subsection{Linux}

\scalaris{} uses autoconf for configuring the build environment and
GNU Make for building the code.

\begin{lstlisting}[language=sh]
%> ./configure
%> make
%> make docs
\end{lstlisting}

For more details read \code{README} in the main \scalaris{} checkout
directory.

\subsection{Windows}

We are currently not supporting \scalaris{} on Windows. However, we
have two small {\tt bat} files for building and running scalaris
nodes. It seems to work but we make no guarantees.

\begin{itemize}
\item Install Erlang\\
       \url{http://www.erlang.org/download.html}
\item Install OpenSSL (for crypto module)\\
       \url{http://www.slproweb.com/products/Win32OpenSSL.html}
\item Checkout scalaris code from SVN
\item adapt the path to your Erlang installation in \code{build.bat}
\item start a \code{cmd.exe}
\item go to the scalaris directory
\item run \code{build.bat} in the cmd window
\item check that there were no errors during the compilation;
       warnings are fine
\item go to the bin sub-directory
\item adapt the path to your Erlang installation in \code{boot.bat},
       \code{cs_local.bat}, \code{cs_local2.bat} and \code{cs_local3.bat}
\item run \code{boot.bat} or one of the other start scripts in the cmd window
\end{itemize}

\code{build.bat} will generate a \code{Emakefile} if there is none yet.
If you have Erlang $<$ R13B04, you will need to adapt the \code{Emakefile}.
There will be empty lines in the first three blocks ending with
``\code{ ]\}.}'': add the following to these lines and try to compile again.
It should work now.

\begin{lstlisting}[language=erlang]
, {d, type_forward_declarations_are_not_allowed}
, {d, forward_or_recursive_types_are_not_allowed}
\end{lstlisting}

For the most recent description please see the FAQ at
\url{http://code.google.com/p/scalaris/wiki/FAQ}.

\subsection{Java-API}

The following commands will build the Java API for \scalaris{}:
\begin{lstlisting}[language=sh]
%> make java
\end{lstlisting}

This will build scalaris.jar, which is the library for accessing
the overlay network. Optionally, the documentation can be build:
\begin{lstlisting}[language=sh]
%> cd java-api
%> ant doc
\end{lstlisting}


\section{Running \scalaris{}}

As mentioned above, in \scalaris{} there are two kinds of nodes:
\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
\item boot servers
\item regular nodes
\end{itemize}

In every \scalaris{}, at least one boot server is required. It will
maintain a list of nodes taking part in the system and allows other
nodes to join the ring. For redundancy, it is also possible to have
several boot servers. In the future, we want to eliminate this
distinction, so any node is also a boot-server.

\subsection{Running on a local machine}
\label{sec.boot}

Open at least two shells. In the first, inside the \scalaris{} directory,
start the boot script (\code{boot.bat} on Windows):
\begin{lstlisting}[language=sh]
%> ./bin/boot.sh
\end{lstlisting}

This will start the boot server. On success \url{http://localhost:8000}
should point to the management interface page of the boot server. The main
page will show you the number of nodes currently in the system. After a
couple of seconds a first \scalaris{} should have started in the boot server
and the number should increase to one. The main page will also allow you to
store and retrieve key-value pairs but should not be used by applications
to access \scalaris{}. See Chapter~\sieheref{chapter.apis} for application
APIs.

%The boot server should show output similar to the following, when
%starting the first \scalaris{} nodes. The first line is printed when
%the \scalaris{} is spawned. Afterwards it will try to connect the
%boot server. When the third line is printed, it managed to contact the
%boot server and joined the ring. In this case, it was the first node
%in the ring.
%\begin{lstlisting}
%[ I | Node   | <0.97.0> ] joining "23947834870"
%[ I | Node   | <0.97.0> ] join as first [50,51,57,52,55,56,51,52,56,55,48]
%[ I | Node   | <0.97.0> ] joined
%\end{lstlisting}

In a second shell, you can now start a second \scalaris{} node. This
will be a `regular server':
\begin{lstlisting}[language=sh]
%> ./bin/cs_local.sh
\end{lstlisting}

The second node will read the configuration file and use this
information to contact the boot server and join the ring. The
number of nodes on the web page should have increased to two by now.

Optionally, a third and fourth node can be started on the same
machine. In a third shell:
\begin{lstlisting}[language=sh]
%> ./bin/cs_local2.sh
\end{lstlisting}


In a fourth shell:
\begin{lstlisting}[language=sh]
%> ./bin/cs_local3.sh
\end{lstlisting}


This will add 3 nodes to the network. The web pages at
\url{http://localhost:8000} should show the additional nodes.

On linux you can also use the \code{scalarisctl} script to start boot and
`regular' nodes.

\subsection{Running distributed}

\scalaris{} can be installed on other machines in the same way as
described in Section~\ref{sec:install}. In the default configuration,
nodes will look for the boot server on \code{localhost} on port 14195. You
should create a \code{scalaris.local.cfg} pointing to the node running
the boot server.

\begin{lstlisting}[language=erlang]
% Insert the appropriate IP-addresses for your setup
% as comma separated integers:
% IP Address, Port, and label of the boot server
{boot_host, {{127,0,0,1},14195,boot}}.
\end{lstlisting}

If you are using the default configuration on the boot server it will
listen on port 14195 and you only have to change the IP address in the
configuration file. Otherwise the other nodes will not find the boot
server. On the remote nodes, you only need to call
\code{./cs_local.sh} and they will automatically contact the
configured boot server.

%\subsection{Running on PlanetLab}

%\subsection{Replication Degree}

%\subsection{Routing Scheme}

\section{Installation}
\label{sec:install}

For simple tests, you do not need to install \scalaris{}. You can run
it directly from the source directory. Note: \code{make install} will
install scalaris into \code{/usr/local} and place \code{scalarisctl}
into \code{/usr/local/bin}. But is more convenient to build an RPM and
install it.

\begin{lstlisting}{language=sh}
svn checkout http://scalaris.googlecode.com/svn/trunk/ scalaris-0.0.1
tar -cvjf scalaris-0.0.1.tar.bz2 scalaris-0.0.1 --exclude-vcs
cp scalaris-0.0.1.tar.bz2 /usr/src/packages/SOURCES/
rpmbuild -ba scalaris-0.0.1/contrib/scalaris.spec
\end{lstlisting}

Your source and binary RPM will be generated in
\code{/usr/src/packages/SRPMS} and \code{RPMS}. We build RPMs and Debs
using checkouts from svn and provide them using the openSUSE BuildService at
\url{http://download.opensuse.org/repositories/home:/tschuett/}. Packages
are available for

\begin{itemize}
\item Fedora 9, 10, 11, 12, 13,
\item Mandriva 2008, 2009, 2009.1, 2010,
\item openSUSE 11.0, 11.1, 11.2, 11.3, Factory,
\item SLE 10, 11,
\item CentOS 5.4,
\item RHEL 5,
\item Debian 5.0 and
\item Ubuntu 9.04, 9.10, 10.04.
\end{itemize}

Inside those repositories you will also find an erlang package - you don't
need this if you already have a recent enough erlang version!

\section{Logging}
\label{sec:logging}
\svnrev{r1083}

\scalaris{} uses the log4erl library (see \code{contrib/log4erl}) for
logging status information and error messages. The log level can be
configured in \code{bin/scalaris.cfg} for both the stdout and file logger.
The default value is {\tt warn}; only warnings, errors and severe problems are
logged.

\begin{lstlisting}[language=erlang]
%% @doc Loglevel: debug < info < warn < error < fatal < none
{log_level, warn}.
{log_level_file, warn}.
\end{lstlisting}

In some cases, it might be necessary to get more complete logging
information, e.g. for debugging. In \sieheref{sec:start-addit-local},
we are explaining the startup process of \scalaris{} nodes in more
detail, here the {\tt info} level provides more detailed information.

\begin{lstlisting}[language=erlang]
%% @doc Loglevel: debug < info < warn < error < fatal < none
{log_level, info}.
{log_level_file, info}.
\end{lstlisting}

\chapter{Using the system}
\label{chapter.apis}

\section{JSON API}

\scalaris{} supports a JSON API for transactions. To minimize the necessary
round trips between a client and \scalaris{}, it uses request lists, which
contain all requests that can be done in parallel. The request list is then
send to a \scalaris{} node with a POST message. The result is an opaque
TransLog and a list containing the results of the requests. To add further
requests to the transaction, the TransLog and another list of requests may
be send to \scalaris{}. This process may be repeated as often as necessary.
To finish the transaction, the request list can contain a 'commit' request
as the last element, which triggers the validation phase of the transaction
processing.

The JSON-API can be accessed via the \scalaris{}-Web-Server running on port
8000 by default and the page \code{jsonrpc.yaws} (For example at:
\url{http://localhost:8000/jsonrpc.yaws}).  The following example
illustrates the message flow:

\begin{longtable}{p{0.45\textwidth}cp{0.45\textwidth}}
\bf Client & & \bf \scalaris{} node \\
Make a transaction, that sets two keys:
\begin{lstlisting}[language=java]
{
  "method":"req_list",
  "version":"1.1",
  "params":
    [
      [
        { "write":{"keyA":"valueA"} },
        { "write":{"keyB":"valueB"} },
        { "commit":"commit" }
      ]
    ],
  "id":0
}
\end{lstlisting}
& $\to$
& \\

& $\leftarrow$ &
\scalaris{} sends results back
\begin{lstlisting}[language=java]
{ "result":
  { "results":
      [
        { "op":"commit",
          "value":"ok",
          "key":"ok" },
        { "op":"write",
          "value":"valueB",
          "key":"keyB" },
        { "op":"write",
          "value":"valueA",
          "key":"keyA" }
      ],
    "translog":
     [...]
  },
  "id" : 0
}
\end{lstlisting}
\\

In a second transaction: Read the two keys
\begin{lstlisting}[language=java]
{
  "method":"req_list",
  "version":"1.1",
  "params":
    [
      [
        { "read":"keyA" },
        { "read":"keyB" }
      ]
    ]
  "id":0
}
\end{lstlisting}
& $\to$ & \\

& $\leftarrow$ &
\scalaris{} sends results back
\begin{lstlisting}[language=java]
{ "result":
  {"results":
    [
      { "op":"read",
        "value":"valueB",
        "key":"keyB" },
      { "op":"read",
        "value":"valueA",
        "key":"keyA" }
    ],
   "translog":
    [...] // this list is the translog
          // for further operations!
          // We name it TLOG here.
  },
  "id" : 0
}
\end{lstlisting}\\

Calculate something with the read values and make further requests, here a
write and the commit for the whole transaction. Also include the latest
translog we got from \scalaris{} (named \code{TLOG} here).

\begin{lstlisting}[language=java]
{
  "method":"req_list",
  "version":"1.1",
  "params":
    [
      TLOG, // translog from prev. result
      [
        { "write":{"keyA":"valueA2"} },
        { "commit":"commit" }
      ]
    ],
  "id" : 0
}
\end{lstlisting}
& $\to$ & \\

& $\leftarrow$ &
\scalaris{} sends results back
\begin{lstlisting}[language=java]
{ "result":
  { "results":
    [ { "op":"commit",
        "value":"ok",
        "key":"ok" },
      { "op":"write",
        "value":"valueA2",
        "key":"keyA" }
    ],
    "translog":
    [...]
  },
  "id" : 0
}
\end{lstlisting}\\
\end{longtable}

A sample usage of the JSON API using Ruby can be found in \code{contrib/jsonrpc.rb}.

A single request list must not contain a key more than once!

The allowed requests are:
\begin{lstlisting}[language=java]
{ "read":"any_key" }

{ "write":{"any_key":"any_value"} }

{ "commit":"commit" }
\end{lstlisting}

The possible results are:
\begin{lstlisting}[language=java]
{ "op":"read", "key":"any_key", "value":"any_value" }
{ "op":"read", "key":"any_value", "fail":"reason" } // 'not_found' or 'timeout'

{ "op":"write",  "key":"any_key", "value":"any_value" }
{ "op":"read", "key":"any_key", "fail":"reason" }

{ "op":"commit", "value":"ok", "key":"ok" }
{ "op":"commit", "value":"fail", "fail":"reason" }
\end{lstlisting}

\subsection{Deleting a key}

Outside transactions keys can also be deleted, but it has to be done with
care, as explained in the following thread on the mailing list:
\url{http://groups.google.com/group/scalaris/browse_thread/thread/ff1d9237e218799}.

\begin{lstlisting}[language=java]
{
  "method":"delete",
  "version":"1.1",
  "params":
    [
      { "key":"any_key" }
    ],
  "id" : 0
}
\end{lstlisting}

Two sample results

\begin{lstlisting}[language=java]
{ "result":
  { "ok":2, // how many replicas were deleted successsfully
    "results": [ "ok", "ok", "locks_set", "undef" ]
  }
}
\end{lstlisting}

\begin{lstlisting}[language=java]
{ "result":
  { "failure":"reason" }
}
\end{lstlisting}


%\section{Erlang}

\section{Java command line interface}

The jar file contains a small command line interface client. For
convenience, we provide a wrapper script called \code{scalaris} which
sets up the Java environment:

\begin{lstlisting}[language=]
%> ./java-api/scalaris -help
Script Options:
  --help, --h            print this message and scalaris help
  --noconfig             suppress sourcing of /etc/scalaris/scalaris-java.conf
                         and $HOME/.scalaris/scalaris-java.conf config files
  --execdebug            print scalaris exec line generated by this
                         launch script
  
usage: scalaris [Options]
 -b,--minibench                   run mini benchmark
 -d,--delete <key> <[timeout]>    delete an item (default timeout: 2000ms)
                                  WARNING: This function can lead to inconsistent data
                                  (e.g. deleted items can re-appear). Also when
                                  re-creating an item the version before the delete can
                                  re-appear.
 -g,--getsubscribers <topic>      get subscribers of a topic
 -h,--help                        print this message
 -lh,--localhost                  gets the local host's name as known to
                                  Java (for debugging purposes)
 -p,--publish <topic> <message>   publish a new message for the given
                                  topic
 -r,--read <key>                  read an item
 -s,--subscribe <topic> <url>     subscribe to a topic
 -u,--unsubscribe <topic> <url>   unsubscribe from a topic
 -v,--verbose                     print verbose information, e.g. the
                                  properties read
 -w,--write <key> <value>         write an item
\end{lstlisting}

\code{read}, \code{write} and \code{delete} can be used to read, write
and delete from/to the overlay, respectively. \code{getsubscribers},
\code{publish}, and \code{subscribe} are the PubSub functions. The others
provide debugging and testing functionality.

\begin{lstlisting}[language=]
%> ./java-api/scalaris -write foo bar
write(foo, bar)
%> ./java-api/scalaris -read foo
read(foo) == bar
\end{lstlisting}

Per default, the \code{scalaris} script tries to connect to a boot server
at \code{localhost}. You can change the node it connects to (and further
connection properties) by adapting the values defined in
\code{java-api/scalaris.properties}.

\section{Java API}

The \code{scalaris.jar} provides the command line client as well as a
library for Java programs to access \scalaris{}. The library provides two
classes:

\begin{itemize}
\item \code{Scalaris} provides a high-level API similar to the command line
       client.
\item \code{Transaction} provides a low-level API to the transaction
       mechanism.
\end{itemize}

For details we refer the reader to the Javadoc:

\begin{lstlisting}[language=sh]
%> cd java-api
%> ant doc
%> firefox doc/index.html
\end{lstlisting}

\chapter{Testing the system}

\section{Running the unit tests}
There are some unit tests in the \code{test} directory. You can call them
by running \code{make test} in the main directory. The results are stored
in a local \code{index.html} file. 

The tests are implemented with the \code{common-test} package from the
Erlang system. For running the tests we rely on \code{run\_test},
which is part of the \code{common-test} package, but (on erlang $<$ R14) is not
installed by default. \code{configure} will check whether \code{run\_test} is
available. If it is not installed, it will show a warning and a short
description of how to install the missing file.

Note: for the unit tests, we are setting up and shutting down several
overlay networks. During the shut down phase, the runtime environment
will print extensive error messages. These error messages do not
indicate that tests failed! Running the complete test suite takes
about 3 minutes, depending on your machine. Only if the complete suite
finishes, it will present statistics on failed and successful tests.

\chapter{Troubleshooting}

\section{Network}

\scalaris{} uses a couple of TCP ports for communication. It does
not use UDP at the moment.

\begin{tabular}{ll}
8000 & HTTP Server on the boot node\\
8001 & HTTP Server on the other nodes\\
14195 & Port for inter-node communication (boot server)\\
14196 & Port for inter-node communication (other nodes)\\
\end{tabular}

Please make sure that at least 14195 and 14196 are not blocked by
firewalls.


\part{Developers Guide}

\chapter{General Hints}

\section{Coding Guidelines}

\begin{itemize}
\item Keep the code short
\item Use \erlmodule{gen\_component} to implement additional processes
\item Don't use receive by yourself (Exception: to implement single threaded
  user API calls (cs\_api, yaws\_calls, etc)
\item Don't use \erlfun{erlang}{now}{/0}, \erlfun{erlang}{send\_after}{/3},
  \code{receive after} etc. in performance critical code, consider
  using \erlmodule{msg\_delay} instead.
\item Don't use \erlfun{timer}{tc}{/3} as it catches exceptions. Use
  \erlfun{util}{tc}{/3} instead.
\end{itemize}

\section{Testing Your Modifications and Extensions}

\begin{itemize}
\item Run the testsuites using \code{make test}
\item Run the java api test using \code{make java-test}
      (\scalaris{} output will be printed if a test fails; if you want to see
      it during the tests, start a \code{bin/boot.sh} and run the tests by
      \code{cd java; ant test})
\item Run the Ruby client by starting Scalaris and running
      \code{cd contrib; ./jsonrpc.rb}
\end{itemize}

\section{Help with Digging into the System}
\label{sec:digging}

\begin{itemize}
\item use \erlfun{ets}{i}{/0,1} to get details on the local state of some
      processes
\item consider changing pdb.erl to use ets instead of erlang:put/get
\item Have a look at strace -f -p PID of beam process
\item Get message statistics via the Web-interface
\item enable/disable tracing for certain modules
%% \item enable gen-component profiling and read the collected data using
%% 
%%    \code{lists:reverse(lists:keysort(2,ets:tab2list(profiling))).}
%% 
%%    (Has the limitation that it measures using absolute time, maybe the data
%%    is more ok, when using -smp disable? ...)
\item Use etop and look at the total memory size and atoms generated
\item send processes sleep or kill messages to test certain behaviour (see
  gen\_component.erl
\item use \code{boot_server:number_of_nodes(). flush().}
\item use \code{admin_checkring(). flush().}
\end{itemize}

\chapter{System Infrastructure}

\section{Groups of Processes}
\label{sec:pid_groups}
\erlmoduleindex{pid\_groups}

\begin{itemize}
\item What is it? How to distinguish from Erlangs internal named processes?
\item Joining a process group
\item Why do we do this... (managing several independent nodes inside a single
  Erlang VM for testing)
\end{itemize}

\section{\texorpdfstring{The Communication Layer \erlmodule{comm}}
          {The Communication Layer comm}}
\label{sec:comm}
\erlmoduleindex{comm}

\begin{itemize}
\item in general
\item format of messages (tuples)
\item use messages with cookies (server and client side)
\item What is a message tag?
\end{itemize}

\input{gen_component}

\section{The Process' Database (\texttt{pdb})}
\erlmoduleindex{pdb}

\begin{itemize}
\item How to use it and how to switch from erlang:put/set to ets and implied
  limitations.
\end{itemize}

\section{Writing Unittests}

\subsection{Plain unittests}

\subsection{Randomized Testing using \texttt{tester.erl}}


\chapter{Basic Structured Overlay}

\section{Ring Maintenance}

\section{T-Man}

\section{Routing Tables}
\label{chapter.routing}
\svnrev{r934}
\erlmoduleindex{rt\_beh}

Each node of the ring can perform searches in the overlay.

A search is done by a lookup in the overlay, but there are several
other demands for communication between peers. \scalaris{} provides
a general interface to route a message to the (other) peer, which is
currently responsible for a given \code{key}.

\codesnippet{lookup.erl}{lookup:lookup}{../src/lookup.erl}

The message \code{Msg} could be a \code{get_key} which retrieves content from
the responsible node or a \code{get_node} message, which returns a pointer
to the node.

All currently supported messages are listed in the file \code{dht_node.erl}.

The message routing is implemented in \code{dht_node_lookup.erl}

\codesnippet{dht_node_lookup.erl}{dht_node_lookup:routing}{../src/dht_node_lookup.erl}

Each node is responsible for a certain key interval. The function
\erlfun{intervals}{in}{/2} is used to decide, whether the key is between
the current node and its successor. If that is the case, the final step is
delivers a \code{lookup_fin} message to the local node. Otherwise, the message
is forwarded to the next nearest known peer (listed in the routing table)
determined by \erlfun{?RT}{next\_hop}{/2}.

\code{rt_beh.erl} is a generic interface for routing tables. It
can be compared to interfaces in Java. In Erlang interfaces can be
defined using a so called `behaviour'.  The files \code{rt_simple} and
\code{rt_chord} implement the behaviour `rt\_beh'.

The macro \code{?RT} is used to select the current implementation of routing
tables. It is defined in \code{include/scalaris.hrl}.

\codesnippet{scalaris.hrl}{scalaris:rt}{../include/scalaris.hrl}

The functions, that have to be implemented for a routing mechanism are
defined in the following file:

\codesnippet{rt_beh.erl}{rt_beh:behaviour}{../src/rt_beh.erl}

\begin{description}
\setlength{\parskip}{0pt}
\setlength{\itemsep}{0pt}
\erlfunindex{rt\_beh}{empty}
\item \code{empty/1} gets a successor and generates an empty routing
  table for use inside the routing table implementation. The data structure of
  the routing table is undefined. It can be a list, a tree, a matrix \ldots

\erlfunindex{rt\_beh}{empty\_ext}
\item \code{empty_ext/1} similarly creates an empty external routing table
  for use by the \erlmodule{dht\_node}. This process might not need all the
  information a routing table implementation requires and can thus work with
  less data.

\erlfunindex{rt\_beh}{hash\_key}
\item \code{hash_key/1} gets a key and maps it into the overlay's
  identifier space.

\erlfunindex{rt\_beh}{get\_random\_node\_id}
\item \code{get_random_node_id/0} returns a random node id from the
  overlay's identifier space. This is used for example when a new node
  joins the system.

\erlfunindex{rt\_beh}{next\_hop}
\item \code{next_hop/2} gets a \erlmodule{dht\_node}'s state (including the
  external routing table representation) and a key and returns the node, that
  should be contacted next when searching for the key, i.e. the known node
  nearest to the id.

\erlfunindex{rt\_beh}{init\_stabilize}
\item \code{init_stabilize/3} is called periodically to rebuild the
  routing table. The parameters are the identifier of the node, its
  successor and the old (internal) routing table state. This method may send
  messages to the \code{routing_table} process which need to be handled by
  the \code{handle_custom_message/2} handler since they are
  implementation-specific.

\erlfunindex{rt\_beh}{update}
\item \code{update/7} is called when the node's ID, predecessor and/or
  successor changes. It updates the (internal) routing table with the (new)
  information.

\erlfunindex{rt\_beh}{filter\_dead\_node}
\item \code{filter_dead_node/2} is called by the failure detector and tells
  the routing table about dead nodes. This function gets the (internal) routing
  table and a node to remove from it. A new routing table state is returned.

\erlfunindex{rt\_beh}{to\_pid\_list}
\item \code{to_pid_list/1} get the PIDs of all (internal) routing table
  entries.

\erlfunindex{rt\_beh}{get\_size}
\item \code{get_size/1} get the (internal or external) routing table's size.

\erlfunindex{rt\_beh}{get\_replica\_keys}
\item \code{get_replica_keys/1} Returns for a given (hashed)
  \code{Key} the (hashed) keys of its replicas. This used for implementing
  symmetric replication.

\erlfunindex{rt\_beh}{n}
\item \code{n/0} gets the number of available keys. An implementation may
  throw \code{throw:not_supported} if the operation is unsupported by
  the routing table.

\erlfunindex{rt\_beh}{dump}
\item \code{dump/1} dump the (internal) routing table state for debugging,
  e.g. by using the web interface. Returns a list of
  \code{\{Index, Node_as_String\}} tuples which may just as well be empty.

\erlfunindex{rt\_beh}{to\_list}
\item \code{to_list/1} convert the (external) representation of the routing
  table inside a given \code{dht_node_state} to a sorted list of known nodes
  from the routing table, i.e. first=succ, second=next known node on the ring,
  \ldots This is used by bulk-operations to create a
  broadcast tree.

\erlfunindex{rt\_beh}{export\_rt\_to\_dht\_node}
\item \code{export_rt_to_dht_node/4} convert the internal routing table state
  to an external state. Gets the internal state, the node's ID, the predecessor
  and the successor for doing so.

\erlfunindex{rt\_beh}{handle\_custom\_message}
\item \code{handle_custom_message/2} handle messages specific to the routing
  table implementation. \erlmodule{rt\_loop} will forward unknown messages
  to this function.

\erlfunindex{rt\_beh}{check}
\item \code{check/5}, \code{check/6} are implemented in
  \code{rt_generic.hrl}, check for routing table changes and send an updated
  (external) routing table to the \erlmodule{dht\_node} process.

\erlfunindex{rt\_beh}{check\_config}
\item \code{check_config/0} check that all required configuration parameters
  exist and satisfy certain restrictions.

\end{description}

\subsection{The routing table process (\texorpdfstring{\code{rt_loop}}{rt\_loop})}
\erlmoduleindex{rt\_loop}

The \erlmodule{rt\_loop} module implements the process for all routing tables.
It processes messages and calls the appropriate methods in the specific routing
table implementations.

\codesnippet{rt_loop.erl}{rt_loop:state}{../src/rt_loop.erl}
If initialized, the node's id, its predecessor, successor and the routing table
state of the selected implementation (the macro \code{RT} refers to).

\codesnippet{rt_loop.erl}{rt_loop:trigger}{../src/rt_loop.erl}
Periodically (see \code{routingtable_trigger} and
\code{pointer_base_stabilization_interval} config parameters) a \code{trigger}
message is sent to the \code{rt_loop} process that starts the periodic
stabilization implemented by each routing table.

\codesnippet{rt_loop.erl}{rt_loop:update_rt}{../src/rt_loop.erl}
Every time a node's neighborhood changes, the \erlmodule{dht\_node} sends an
\code{update_rt} message to the routing table which will call
\erlfun{?RT}{update}{/7} that decides whether the routing table should be
re-build. If so, it will stop any waiting trigger and schedule an immideate
(periodic) stabilization.

\subsection{Common methods for routing table implementations
(\texorpdfstring{\code{rt_generic.hrl}}{rt\_generic.hrl})}
\erlmoduleindex{rt\_generic}

This file can be included by any routing table implementation and provides
a default implementation of the \code{check/5} and \code{check/6} functions.

\codesnippet{rt_generic.hrl}{rt_generic:check}{../src/rt_generic.hrl}
Checks whether the routing table changed and in this case sends the
\erlmodule{dht\_node} an updated (external) routing table state. Optionally
the failure detector is updated. This may not be necessary, e.g. if
\code{check} is called after a crashed node has been reported by the failure
detector (the failure detector already unsubscribes the node in this case).

\subsection{Simple routing table (\texorpdfstring{\code{rt_simple}}{rt\_simple})}
\erlmoduleindex{rt\_simple}

One implementation of a routing table is the \code{rt_simple}, which routes
via the successor. Note that this is inefficient as it needs a linear number
of hops to reach its goal. A more robust implementation, would use a successor
list. This implementation is also not very efficient in the presence of churn.

\subsubsection{Data types}
First, the data structure of the routing table is defined:

\codesnippet{rt_simple.erl}{rt_simple:types}{../src/rt_simple.erl}
The routing table only consists of a node (the successor). Keys in the overlay
are identified by integers $\geq 0$.

\subsubsection{A simple \texorpdfstring{\erlmodule{rm\_beh}}{rm\_beh} behaviour}

\erlfunindex{rt\_simple}{empty}
\codesnippet{rt_simple.erl}{rt_simple:empty}{../src/rt_simple.erl}
\erlfunindex{rt\_simple}{empty\_ext}
\codesnippet{rt_simple.erl}{rt_simple:empty_ext}{../src/rt_simple.erl}
The empty routing table (internal or external)  consists of the successor.

\erlfunindex{rt\_simple}{hash\_key}
\codesnippet{rt_simple.erl}{rt_simple:hash_key}{../src/rt_simple.erl}
Keys are hashed using MD5 and have a length of 128 bits.

\erlfunindex{rt\_simple}{get\_random\_node\_id}
\codesnippet{rt_simple.erl}{rt_simple:get_random_node_id}{../src/rt_simple.erl}
Random node id generation uses the helpers provided by the \erlmodule{randoms}
module.

\erlfunindex{rt\_simple}{next\_hop}
\codesnippet{rt_simple.erl}{rt_simple:next_hop}{../src/rt_simple.erl}
Next hop is always the successor.

\erlfunindex{rt\_simple}{init\_stabilize}
\codesnippet{rt_simple.erl}{rt_simple:init_stabilize}{../src/rt_simple.erl}
\code{init_stabilize/3} resets its routing table to the current successor.

\erlfunindex{rt\_simple}{update}
\codesnippet{rt_simple.erl}{rt_simple:update}{../src/rt_simple.erl}
\code{update/7} updates the routing table with the new successor.

\erlfunindex{rt\_simple}{filter\_dead\_node}
\codesnippet{rt_simple.erl}{rt_simple:filter_dead_node}{../src/rt_simple.erl}
\code{filter_dead_node/2} does nothing, as only the successor is listed in
the routing table and that is reset periodically in \code{init_stabilize/3}.

\erlfunindex{rt\_simple}{to\_pid\_list}
\codesnippet{rt_simple.erl}{rt_simple:to_pid_list}{../src/rt_simple.erl}
\code{to_pid_list/1} returns the pid of the successor.

\erlfunindex{rt\_simple}{get\_size}
\codesnippet{rt_simple.erl}{rt_simple:get_size}{../src/rt_simple.erl}
The size of the routing table is always \code{1}.

\erlfunindex{rt\_simple}{get\_replica\_keys}
\codesnippet{rt_simple.erl}{rt_simple:get_replica_keys}{../src/rt_simple.erl}
This \code{get_replica_keys/1} implements symmetric replication.

\erlfunindex{rt\_simple}{n}
\codesnippet{rt_simple.erl}{rt_simple:n}{../src/rt_simple.erl}
There are $2^{128}$ available keys.

\erlfunindex{rt\_simple}{dump}
\codesnippet{rt_simple.erl}{rt_simple:dump}{../src/rt_simple.erl}
\code{dump/1} lists the successor.

\erlfunindex{rt\_simple}{to\_list}
\codesnippet{rt_simple.erl}{rt_simple:to_list}{../src/rt_simple.erl}
\code{to_list/1} lists the successor from the external routing table state.

\erlfunindex{rt\_simple}{export\_rt\_to\_dht\_node}
\codesnippet{rt_simple.erl}{rt_simple:export_rt_to_dht_node}{../src/rt_simple.erl}
\code{export_rt_to_dht_node/1} states that the external routing table is the
same as the internal table.

\erlfunindex{rt\_simple}{handle\_custom\_message}
\codesnippet{rt_simple.erl}{rt_simple:handle_custom_message}{../src/rt_simple.erl}
Custom messages could be send from a routing table process on one node to the
routing table process on another node and are independent from any other
implementation.

\subsection{Chord routing table (\texorpdfstring{\code{rt_chord}}{rt\_chord})}
\erlmoduleindex{rt\_chord}

The file \code{rt_chord.erl} implements Chord's routing.

\subsubsection{Data types}

\codesnippet{rt_chord.erl}{rt_chord:types}{../src/rt_chord.erl}

The routing table is a \code{gb_tree}. Identifiers in the ring are
integers. Note that in Erlang integer can be of arbitrary
precision. For Chord, the identifiers are in $[0, 2^{128})$,
i.e. 128-bit strings.

\subsubsection{The \texorpdfstring{\erlmodule{rm\_beh}}{rm\_beh} behaviour for Chord (excerpt)}

\erlfunindex{rt\_chord}{empty}
\codesnippet{rt_chord.erl}{rt_chord:empty}{../src/rt_chord.erl}
\erlfunindex{rt\_chord}{empty\_ext}
\codesnippet{rt_chord.erl}{rt_chord:empty_ext}{../src/rt_chord.erl}
\code{empty/1} returns an empty \code{gb_tree}, same for \code{empty_ext/1}.

\erlfun{rt\_chord}{hash\_key}{/1},
\erlfun{rt\_chord}{get\_random\_node\_id}{/0},
\erlfun{rt\_chord}{get\_replica\_keys}{/1} and
\erlfun{rt\_chord}{n}{/0} are implemented like their counterparts in
\code{rt_simple.erl}.

\erlfunindex{rt\_chord}{next\_hop}
\codesnippet{rt_chord.erl}{rt_chord:next_hop}{../src/rt_chord.erl}
If the (external) routing table contains at least one item, the next hop is
retrieved from the \code{gb_tree}. It will be the node with the largest id
that is smaller than the id we are looking for. If the routing table is empty,
the successor is chosen. However, if we haven't found the key in our routing
table, the next hop will be our largest finger, i.e. entry.

\erlfunindex{rt\_chord}{init\_stabilize}
\codesnippet{rt_chord.erl}{rt_chord:init_stabilize}{../src/rt_chord.erl}
The routing table stabilization is triggered for the first index and
then runs asynchronously, as we do not want to block the
\code{rt_loop} to perform other request while recalculating the
routing table.

We have to find the node responsible for the calculated finger and therefore
perform a lookup for the node with a \code{rt_get_node} message, including
a reference to ourselves as the reply-to address and the index to be set.

The lookup performs an overlay routing by passing the massage until
the responsible node is found. There, the message is delivered to the
\erlmodule{dht\_node}. At the destination the message is handled in
\code{dht_node.erl}:

\codesnippet{dht_node.erl}{dht_node:rt_get_node}{../src/dht_node.erl}

The remote node sends the requested information back directly. It includes a
reference to itself in a \code{rt_get_node_response} message which will be
handled by \erlfun{rt\_chord}{handle\_custom\_message}{/2} that calls
\erlfun{rt\_chord}{stabilize}{/5}:

\erlfunindex{rt\_chord}{handle\_custom\_message}
\codesnippet{rt_chord.erl}{rt_chord:handle_custom_message}{../src/rt_chord.erl}
\erlfunindex{rt\_chord}{stabilize}
\codesnippet{rt_chord.erl}{rt_chord:stabilize}{../src/rt_chord.erl}

\code{stabilize/5} assigns the received routing table entry and triggers the
routing table stabilization for the the next shorter entry using the same
mechanisms as described above.

If the shortest finger is the successor, then filling the routing table is
stopped, as no further new entries would occur. It is not necessary, that
\code{Index} reaches 1 to make that happen. If less than $2^{128}$ nodes
participate in the system, it may happen earlier.

\erlfunindex{rt\_chord}{update}
\codesnippet{rt_chord.erl}{rt_chord:update}{../src/rt_chord.erl}
Tells the \erlmodule{rt\_loop} process to rebuild the routing table starting
with an empty (internal) routing table state.

\erlfunindex{rt\_chord}{filter\_dead\_node}
\codesnippet{rt_chord.erl}{rt_chord:filter_dead_node}{../src/rt_chord.erl}
\code{filter_dead_node} removes dead entries from the \code{gb_tree}.

\erlfunindex{rt\_chord}{export\_rt\_to\_dht\_node}
\codesnippet{rt_chord.erl}{rt_chord:export_rt_to_dht_node}{../src/rt_chord.erl}
\code{export_rt_to_dht_node} converts the internal \code{gb_tree} structure
based on indices into the external representation optimised for look-ups, i.e.
a \code{gb_tree} with node ids and the nodes themselves.


\section{Local Datastore}

\section{Cyclon}

\section{Vivaldi Coordinates}

\section{Estimated Global Information (Gossiping)}

\section{Load Balancing}

\section{Broadcast Trees}



\chapter{Transactions in \scalaris{}}

\section{The Paxos Module}

\section{Transactions using Paxos Commit}

\section{Applying the Tx-Modules to replicated DHTs}

Introduces transaction processing on top of a Overlay



\chapter{How a node joins the system}

\section{General Erlang server loop}

Servers in Erlang often use the following structure to maintain a state
while processing received messages:

\lstset{language=erlang}
\begin{lstlisting}
loop(State) ->
  receive
    Message ->
      State1 = f(State),
      loop(State1)
  end.
\end{lstlisting}

The server runs an endless loop, that waits for a message, processes it and
calls itself using tail-recursion in each branch. The loop works on a
\code{State}, which can be modified when a message is handled.

\section{Starting additional local nodes after boot}
\label{sec:start-addit-local}
\svnrev{r1093}

After booting a new \scalaris{}-System as described in
Section~\sieheref{sec.boot}, ten additional local nodes can be started
by typing \code{admin:add_nodes(10)} in the Erlang-Shell that the boot
process opened~\footnote{Increase the log level to {\tt info} to get
more detailed startup logs. See Section~\sieheref{sec:logging}}.


\lstset{language=erlang}
\begin{lstlisting}
scalaris> ./bin/boot.sh 
[...]
(boot@csr-pc9)1> admin:add_nodes(10)
\end{lstlisting}

In the following we will trace what this function does in order to add
additional nodes to the system.
The function \code{admin:add_nodes(int)} is defined as follows.

\codesnippet{admin.erl}{admin:add_nodes}{../src/admin.erl}

It calls \erlfun{admin}{add\_node}{[]} Count times. This function starts a new
child with the given options for the main supervisor \code{main_sup}.
As defined by the parameters, to actually perform the start, the
function \erlfun{sup\_dht\_node}{start\_link}{/1} is called by the Erlang
supervisor mechanism.  For more details on the OTP supervisor
mechanism see Chapter~18 of the Erlang book~\cite{erlang-book} or the
online documentation at
\url{http://www.erlang.org/doc/man/supervisor.html}.

\subsection{Supervisor-tree of a \scalaris{} node}

When a new Erlang VM with a Scalaris node is started, a
\erlmodule{sup\_scalaris} supervisor is started that creates further
workers and supervisors according to the following scheme
(processes starting order: left to right, top to bottom):

\medskip
{\centering\includegraphics[width=\linewidth]{supervision}}

When new nodes are started using \erlfun{admin}{add\_node}{/1}, only new
\code{sup_dht_node} supervisors are started.

\subsection{Starting the sup\_dht\_node supervisor and general processes of a node}

Starting supervisors is a two step process: a call to
\erlfun{supervisor}{start\_link}{/2,3}, e.g. from a supervisor's own
\code{start_link} method, will start the supervisor process. It will then
call \erlfun[]{Module}{init}{/1} to find out about the restart strategy,
maximum restart frequency and child processes.
Note that \erlfun{supervisor}{start\_link}{/2,3} will not return until
\erlfun[]{Module}{init}{/1} has returned and all child processes have been
started.

Let's have a look at \erlfun{sup\_dht\_node}{init}{/1}, the 'DHT node
 supervisor'.

\codesnippet{sup_dht_node.erl}{sup_dht_node:init}{../src/sup_dht_node.erl}


The return value of the \code{init/1} function specifies the child
processes of the supervisor and how to start them. Here, we define a
list of processes to be observed by a \code{one_for_one}
supervisor. The processes are:
\code{Delayer},
\code{Reregister},
\code{DeadNodeCache},
\code{IdHolder},
\code{RoutingTable},
\code{SupDHTNodeCore_AND},
\code{Cyclon},
\code{RingMaintenance},
\code{Vivaldi},
\code{DC_Clustering} and a
\code{Gossip} process in this order.

The term \code{\{one_for_one, 10, 1\}} specifies that the supervisor
should try 10 times to restart each process before giving
up. \code{one_for_one} supervision means, that if a single process
stops, only that process is restarted. The other processes run
independently.

The \erlfun{sup\_dht\_node}{init}{/1} is finished and the supervisor module,
starts all the defined processes by calling the functions that were
defined in the returned list.

For a join of a new node, we are only interested in the starting of
the \code{SupDHTNodeCore_AND} process here. At that point in time, all
other defined processes are already started and running.

\subsection{Starting the sup\_dht\_node\_core supervisor with a peer and some paxos processes}

Similarly, the supervisor will call the
\erlfun[]{sup\_dht\_node\_core}{init}{/1} function:

\codesnippet{sup_dht_node_core.erl}{sup_dht_node_core:init}{../src/sup_dht_node_core.erl}

It defines five processes, that have to be observed using an
\code{one_for_all}-supervisor, which means, that if one fails, all have to
be restarted. Passed to the \code{init} function is the processes' group
that is used with the \erlmodule{pid\_groups} module and a list of options for
the \erlmodule{dht\_node}. The process group name was calculated
a bit earlier in the code. Exercise: Try to find where.


\codesnippet{dht_node.erl}{dht_node:start_link}{../src/dht_node.erl}

\erlmodule{dht\_node} implements the \code{gen_component} behaviour. This
component was developed by us to enable us to write code which is
similar in syntax and semantics to the examples in~\cite{rachid-book}.
Similar to the \code{supervisor} behaviour, the component has to
provide an \code{init/1} function, but here it is used to initialize the
state of the component. This function is described in the next
section.

Note: \code{?MODULE} is a predefined Erlang macro, which expands to the
module name, the code belongs to (here: \code{dht_node}).

\subsection{\texorpdfstring{Initializing a \code{dht_node}-process}
             {Initializing a dht\_node-process}}

\codesnippet{dht_node.erl}{dht_node:start}{../src/dht_node.erl}

The \code{gen_component} behaviour registers the \code{dht_node} in the
process dictionary. Formerly, the process had to do this itself, but
we moved this code into the behaviour. If the \code{dht_node} is the
first node, it will start immediately by triggering all known nodes
(to initialize the comm layer) and entering the join process
accordingly. The node also retrieves its \code{Id} from the idholder:
\erlfun{idholder}{get\_id}{}. In the first call, a random identifier is
returned, otherwise the latest set value. If the \code{dht_node}-process
failed and is restarted by its supervisor, this call to the idholder
ensures, that the node still keeps its \code{Id}, assuming that the
idholder process is not failing. This is important for the load-balancing
and for consistent responsibility of nodes to ensure consistent lookup in
the structured overlay.

If a node changes its position in the ring for load-balancing, the
idholder will be informed and the \code{dht_node} finishes itself. This
triggers a restart of the corresponding database process via the
and-supervisor. When the supervisor restarts both processes, they will
retrieve the new position in the ring from the idholder and join the ring
there.

\todo{The supervisor was configured to restart a node at most 10 times. Does
  that mean, that a node can only change its position in the ring 10 times
  (caused by load-balancing)?}

\subsection{Actually joining the ring}

After retrieving its identifier, the node starts the join protocol
which processes the appropriate messages calling
\erlfun{dht\_node\_join}{process\_join\_msg}{Message, State}.

\codesnippet{dht_node_join.erl}{dht_node_join:join_first}{../src/dht_node_join.erl}

If the ring is empty, the joining node is the only node in the ring
and will be responsible for the whole key space. \code{join_first}
just creates a new state for a \scalaris{} node consisting of an empty
routing table, a successorlist containing itself, itself as its
predecessor, a reference to itself, its responsibility area from
\code{Id} to \code{Id} (the full ring), and a load balancing schema.

The state is defined in

\codesnippet{dht_node_state.erl}{dht_node_state:state}{../src/dht_node_state.erl}

If a node joins an existing ring, it will at first try to contact all
\code{dht_node} processes in any VM configured in \code{known_hosts}.

\codesnippet{dht_node_join.erl}{dht_node_join:join_other_p12}{../src/dht_node_join.erl}

These nodes will be send a lookup request for the node currently responsible
for the new node's id -- the successor for the joining node. If this lookup
fails for some reason, it is tried again.

\codesnippet{dht_node_join.erl}{dht_node_join:join_other_p3}{../src/dht_node_join.erl}

If its (future) successor is found, this new node will send a \code{join_request}
message including a reference to itself and the chosen \code{Id}.
This message is received by the old node in \code{dht_node.erl}

\codesnippet{dht_node.erl}{dht_node:join_message}{../src/dht_node.erl}

and finally triggers a call to \erlfun{dht\_node\_join}{join\_request}{/2}
on the old node:

\codesnippet{dht_node_join.erl}{dht_node_join:join_request1}{../src/dht_node_join.erl}
\codesnippet{dht_node_join.erl}{dht_node_join:join_request}{../src/dht_node_join.erl}

The \code{dht_node} will update the interval it is responsible for and
notify the ring maintenance of its new predecessor. It will also remove all
key-value pairs from its database which are now in the responsibility of
the joining node and send a \code{join_response} message to the new node
with its former predecessor and the data the new node has to host.

\codesnippet{dht_node_join.erl}{dht_node_join:join_other_p4}{../src/dht_node_join.erl}

Back on the joining node: it waits for the \code{join_response}
message in phase 4 of the join protocol. The next steps after the
message is received from the old node are to initialize the
maintenance components for the ring and routing table, the database
and the state of the \code{dht_node}.
The \erlfun{cs\_replica\_stabilization}{recreate\_replicas}{/1} function is
called, which is not yet implemented. It would recreat necessary replicas
that were lost due to load-balancing, node failures and lost updates during
the data transfer.

The macro \code{?RT} maps to the configured routing algorithm and
\code{?RM} to the configured ring maintenance algorithm. It is defined
in \code{include/scalaris.hrl}. For further details on the routing see
Chapter~\sieheref{chapter.routing}.

Note that join-related messages arriving in other phases than those handling
them will be ignored. Any other messages during a \code{dht_node}'s join will
be queued and re-send when the join is complete.

\todo{What, if the \code{Id} is exactly the same as that of the existing
  node? This could lead to lookup and responsibility inconsistency? Can this
  be triggered by the load-balancing? This is a bug, that should be
  fixed!!!}


\chapter{Directory Structure of the Source Code}

The directory tree of \scalaris{} is structured as follows:

\vspace*{1em}
\begin{tabular}{|r|p{11.5cm}|}
 \hline
 \code{bin} & contains shell scripts needed to work with \scalaris{} (e.g.\ start the boot services, start a node, \dots)\\
 \code{contrib} & necessary third party packages (yaws and log4erl) \\
 \code{doc} & generated Erlang documentation \\
 \code{docroot} & root directory of the node's webserver \\
 \code{ebin} & the compiled Erlang code (beam files)\\
 \code{java-api} & a Java API to \scalaris{} \\
 \code{log} & log files \\
 \code{src} & contains the \scalaris{} source code\\
 \code{test} & unit tests for \scalaris{} \\
 \code{user-dev-guide} & contains the sources for this document\\
 \hline
\end{tabular}

%\chapter{System Components}



%\chapter{Processes}

%\chapter{Troubleshooting}

%\section{ApplicationMonitor appmon:start()}

\chapter{Java API}

For the Java API documentation, we refer the reader to the documentation
generated by javadoc or doxygen. The following commands create the
documentation:

\begin{lstlisting}[language=sh]
%> cd java-api
%> ant doc
%> doxygen
\end{lstlisting}

The documentation can then be found in \code{java-api/doc/index.html}
(javadoc) and\\ \code{java-api/doc-doxygen/html/index.html} (doxygen).

We provide two kinds of APIs:

\begin{itemize}
\item high-level access with \code{de.zib.scalaris.Scalaris}
\item low-level access with \code{de.zib.scalaris.Transaction}
\end{itemize}

The former provides general functions for reading, writing and deleting
single key-value pairs and an API for the built-in PubSub-service. The
latter allows the user to write custom transactions which can modify an
arbitrary number of key-value pairs within one transaction.

\bibliographystyle{plainnat}

\begin{thebibliography}{9}

\bibitem{erlang-book}
Joe Armstrong.
\newblock \emph{Programming Erlang: Software for a Concurrent World.}
\newblock Pragmatic Programmers, ISBN: 978-1-9343560-0-5, July 2007

\bibitem{rachid-book}
Rachid Guerraoui and Luis Rodrigues.
\newblock \emph{Introduction to Reliable Distributed Programming.}
\newblock Springer-Verlag, 2006.

\end{thebibliography}

\printindex

\end{document}
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 

