% Copyright 2007-2011 Zuse Institute Berlin

% Licensed under the Apache License, Version 2.0 (the "License");
% you may not use this file except in compliance with the License.
% You may obtain a copy of the License at
%
%     http://www.apache.org/licenses/LICENSE-2.0
%
% Unless required by applicable law or agreed to in writing, software
% distributed under the License is distributed on an "AS IS" BASIS,
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
% See the License for the specific language governing permissions and
% limitations under the License.
\newcommand{\scalaris}{Scalaris}
\newcommand{\doctitle}{\scalaris{}: Users and Developers Guide}
\newcommand{\docauthor}{Zuse Institute Berlin, onScale solutions}
\newcommand{\docsubject}{\scalaris{}}
\newcommand{\doccreator}{LaTeX2e and pdfLaTeX with hyperref-package}
\newcommand{\docproducer}{}
\newcommand{\dockeywords}{\scalaris{}, P2P, DHT, Manual}
\newcommand{\doccopyright}{Copyright 2007-2011 Zuse Institute Berlin and onScale solutions}
\newcommand{\doclicenseurl}{http://www.apache.org/licenses/LICENSE-2.0}
\newcommand{\docversion}{\input{../VERSION}}

\title{\doctitle}

\documentclass[a4paper]{scrreprt}
\usepackage{typearea}
\areaset[1cm]{165mm}{240mm}

\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc}

\usepackage{relsize}
\usepackage{graphicx}
%\usepackage{color}
%\usepackage{colortbl}
\usepackage{pifont} % for carriage return symbol
\usepackage{longtable}
\usepackage{array}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{makeidx}
\usepackage{ifthen}
\usepackage{fancyhdr}
\usepackage{fancyvrb}
\usepackage{lastpage}
\usepackage{relsize}
\usepackage{xcolor}
\usepackage{threeparttable}

\input{pdfoptions}

\usepackage{listings}
\usepackage{etextools}
\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{shadows}

\renewcommand{\headrulewidth}{0pt}    % Width of head rule
\renewcommand{\footrulewidth}{0.3pt}  % Width of head rule

% normal pages
\pagestyle{fancy}
\fancyhf{} % clear all header and footer fields
%\fancyhead[RE,LO]{}
\fancyhead[R]{}%
\fancyfoot[R]{\bfseries\thepage\ / \pageref{LastPage}}%
\chead{}%
\cfoot{}%

% beginning of a chapter
\fancypagestyle{plain}{%
\fancyhf{} % clear all header and footer fields
%\fancyhead[RE,LO]{}
\fancyhead[R]{}%
\fancyfoot[R]{\bfseries\thepage\ / \pageref{LastPage}}%
\chead{}%
\cfoot{}%
}

% Clear Header Style on the Last Empty Odd pages
\makeatletter
\def\cleardoublepage{\clearpage\if@twoside \ifodd\c@page\else%
    \hbox{}%
    \thispagestyle{empty}%              % Empty header styles
    \newpage%
    \if@twocolumn\hbox{}\newpage\fi\fi\fi}
\makeatother

%% Bold typewriter font
%\renewcommand{\ttdefault}{pcr}
%\renewcommand{\rmdefault}{ptm} % Times
%\renewcommand{\rmdefault}{ppl} % Palatino
%%\renewcommand{\rmdefault}{pnc} % NewCenturySchoolbook
%%\renewcommand{\rmdefault}{pbk} % BookMan
%\renewcommand{\rmdefault}{pag} % Avantgarde (sans serif)
%% \renewcommand{\rmdefault}{pzc} % ZapfChancery (slanted)
%\renewcommand{\rmdefault}{put} % Utopia
\renewcommand{\rmdefault}{bch} % CharterBT

\newcommand{\carriagereturn}{\scriptsize\Pisymbol{psy}{191}}


%%% colors %%%%%%%%%%%%%%%%%%%%%%%%

\definecolor{lightyellow}{rgb}{1.0, 1.0, 0.5}
\definecolor{rltred}{rgb}{0.75,0,0}
\definecolor{rltgreen}{rgb}{0,0.5,0}


%\definecolor{rltblue}{rgb}{0,0,0.75}
\definecolor{rltblue}{HTML}{1F4980}
\definecolor{lightgray}{gray}{0.9}

\setlength{\parindent}{0pt}

% 31 73 128 blue

\definecolor{lightyellow}{rgb}{1.0, 1.0, 0.5}
\definecolor{codebackground}{rgb}{0.9,0.95,1.0}
\definecolor{commandinput}{rgb}{0.8,0.8,1}

%\thicklines
\lstset{
  basicstyle=\scriptsize\ttfamily,
  backgroundcolor=\color{codebackground},
  keywordstyle=\color{blue}\bfseries,
  % underlined bold black keywords
  identifierstyle=\bfseries, % nothing happens
  commentstyle=\color{red}\bfseries, % white comments
  stringstyle=\sffamily, % typewriter type for strings
  showstringspaces=false,
  xleftmargin=3pt,
  xrightmargin=3pt,
  fancyvrb=true,
  frame=single,
%  frameround=tttt,
%  framexleftmargin=0pt,
  framextopmargin=3pt,
  framexbottommargin=3pt,
%  framexrightmargin=5pt,
  rulecolor=\color{codebackground},
  language=erlang,
%  fillcolor=\color{red},
%  rulesepcolor=\color{black}
%  rulesep=1cm,
}
\lstset{rangebeginprefix=\%\%\ userdevguide-begin\ }
\lstset{rangeendprefix=\%\%\ userdevguide-end\ }
\lstset{includerangemarker=false}

% \codesnippet[lstsettings]{filename-label}{range-label}{src-file with path}
% \codesnippet[language=erlang]{admin.erl}{admin:add_nodes}{../src/admin.erl}
\newcommand{\codesnippet}[4][language=erlang]{
{%% File: \url{#4}
\lstset{numbers=left}
\lstinputlisting[
  title=\filetitle{#2},
  linerange=#3-#3,
  #1
]
{#4}
}
}

% \codefile[lstsettings]{filename-label}{src-file with path}
% \codefile[language=erlang]{admin.erl}{../src/admin.erl}
\newcommand{\codefile}[3][language=erlang]{
{
\lstinputlisting[
  title=\filetitle{#2},
  #1]
{#3}
}
}
\newcommand{\bfref}[1]{\textbf{\hyperpage{#1}}}
\newcommand{\sieheref}[1]{\ref{#1} on page~\pageref{#1}}
\newcommand{\code}[1]{\lstinline[basicstyle=\ttfamily]!#1!}
\newcommand{\filetitle}[1]{\hbox to \linewidth{~~File \code{#1:}\hfill}}
\newcommand{\todo}[1]{{\color{red}{\em TODO: #1}}}
\newcommand{\svnrev}[1]
{\hfill\emph{Description is based on SVN revision #1.}\medskip}

\newcommand{\erlparamsorarity}[1]{%
\ifthenelse{\equal{0}{\gettokslistindex{/}{#1}}}{\texttt{#1}}{(\texttt{#1})}}
\newcommand{\erlfun}[4][index]{%
\ifthenelse{\equal{index}{#1}}%
{\index{#2@\texttt{#2}!#3@\texttt{#3}}}%
{}%
\texttt{#2}:\-\texttt{#3}\texttt{\erlparamsorarity{#4}}}

\newcommand{\erlfunindex}[2]{%
\index{#1@\texttt{#1}!#2@\texttt{#2}|bfref}}%

\newcommand{\erlmodule}[2][index]{%
\ifthenelse{\equal{index}{#1}}%
{\index{#2@\texttt{#2}}}{}%
\texttt{#2}}

\newcommand{\erlmoduleindex}[1]{%
\index{#1@\texttt{#1}|bfref}}%

\makeatletter
\newenvironment{erlfunparams}
{
\list{}{
    \labelwidth\z@ \makeatother \itemindent-\leftmargin
    \let\makelabel\descriptionlabel
\setlength\leftmargin{4em}
\setlength{\parskip}{0pt}
\setlength{\parsep}{0pt}
\setlength{\itemsep}{0pt}
}}
{\endlist}
\makeatother
\newcommand{\erlparam}[2]{\item[\texttt{#1}:] #2}

\newenvironment{wikitext}
{\catcode`\#=13\catcode`\$=13\catcode`\_=11\catcode`\==13
}{\catcode`\#=6\catcode`\$=3\catcode`\_=8}

% new left-justified column types
\newcolumntype{P}[1]{>{\raggedright}p{#1}}
\newcolumntype{M}[1]{>{\raggedright}m{#1}}
\newcolumntype{B}[1]{>{\raggedright}m{#1}}
\newcommand{\tn}{\tabularnewline} % need to use this instead of \\ in these columns

% allow footnotes in tables:
\usepackage{footnote}
\makesavenoteenv{tabular}

\makeindex

\begin{document}
\vspace*{4cm}
\thispagestyle{empty}
\setlength{\parskip}{1ex}
\begin{tabular}{p{4cm}p{0.5cm}p{10cm}}
%\vspace*{-5cm}\includegraphics[width=5cm]{scalaris-layers}
\parbox{4cm}{\includegraphics[width=4cm]{scalaris-layers}}
& &\sffamily\bfseries\Huge
  \bigskip {\textcolor{rltblue}{\scalaris{}:}}

\medskip
 \mdseries Users and Developers Guide

\bigskip\medskip
\LARGE Version \docversion{} \hfill \today\\
\end{tabular}
\vfill
{\scriptsize
\doccopyright{}.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

\url{\doclicenseurl}

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
}

\tableofcontents

\part{Users Guide}

\chapter{Introduction}

\scalaris{} is a scalable, transactional, distributed key-value store based on
the peer-to-peer principle. It can be used to build scalable Web 2.0
services. The concept of \scalaris{} is quite simple: Its architecture consists
of three layers.

It provides self-management and scalability by replicating services and data
among peers. Without system interruption it scales from a few PCs to
thousands of servers. Servers can be added or removed on the fly without any
service downtime.

\begin{center}
\includegraphics[width=0.7\linewidth]{layers}
\end{center}

\scalaris{} takes care of:

\begin{itemize}
\item Fail-over
\item Data distribution
\item Replication
\item Strong consistency
\item Transactions
\end{itemize}

The \scalaris{} project was initiated by Zuse Institute Berlin and onScale
solutions and was partly funded by the EU projects Selfman and
XtreemOS. Additional information (papers, videos) can be found at
\url{http://www.zib.de/CSR/Projects/scalaris} and
\url{http://www.onscale.de/scalarix.html}.

\section{Brewer's CAP Theorem}

In distributed computing there exists the so called CAP theorem. It
basically says that there are three desirable properties for distributed
systems but one can only have any two of them.

\begin{description}
\item {Strict Consistency.} Any read operation has to return the
  result of the latest write operation on the same data item.

\item {Availability.} Items can be read and modified at any time.

\item {Partition Tolerance.} The network on which the service is
  running may split into several partitions which cannot communicate
  with each other. Later on the networks may re-join again.

  For example, a service is hosted on one machine in Seattle and one
  machine in Berlin. This service is partition tolerant if it can
  tolerate that all Internet connections over the Atlantic (and
  Pacific) are interrupted for a few hours and then get repaired.
\end{description}

The goal of \scalaris{} is to provide strict consistency and partition
tolerance. We are willing to sacrifice availability to make sure that
the stored data is always consistent. I.e. when you are running
\scalaris{} with a replication degree of 4 and the network splits into
two partitions, one partition with three replicas and one partition
with one replica, you will be able to continue to use the service only
in the larger partition. All requests in the smaller partition will
time out until the two networks merge again. Note, most other
key-value stores tend to sacrifice consistency.

\section{Scientific Background}

{\bf Basics.} The general structure of \scalaris{} is modelled after
Chord. The Chord paper~\cite{chord-sigcomm} describes the ring
structure, the routing algorithms, and basic ring maintenance.

The main routines of our Chord node are in \code{src/dht\_node.erl} and
the join protocol is implemented in \code{src/dht\_node\_join.erl} (see
also Chap.~\sieheref{chapter.join}). Our implementation of the routing
algorithms is described in more detail in Sect.~\sieheref{chapter.routing}
and the actual implementation is in \code{src/rt\_chord.erl}.

{\bf Transactions.} The most interesting part is probably the
transaction algorithms. The most current description of the algorithms
and background is in \cite{enhanced-paxos}.

The implementation consists of the paxos algorithm in \code{src/paxos} and
the transaction algorithms itself in \code{src/transactions} (see also
Chap.~\sieheref{chapter.transactions}).

{\bf Ring Maintenance.} We changed the ring maintenance algorithm in
\scalaris{}. It is not the standard Chord one, but a variation of
T-Man~\cite{t-man}. It is supposed to fix the ring structure
faster. In some situations, the standard Chord algorithm is not able
to fix the ring structure while T-Man can still fix it. For node sampling,
our implementation relies on Cyclon~\cite{cyclon}.

The T-Man implementation can be found in \code{src/rm\_tman.erl} and
the Cyclon implementation in \code{src/cyclon}.

{\bf Vivaldi Coordinates.} For some experiments, we implemented so
called Vivaldi coordinates~\cite{vivaldi}. They can be used to
estimate the network latency between arbitrary nodes.

The implementation can be found in \code{src/vivaldi.erl}.

{\bf Gossipping.} For some algorithms, we use estimates of global
information. These estimates are aggregated with the help of
gossipping techniques~\cite{gossip}.

The implementation can be found in \code{src/gossip.erl}.



\chapter{Download and Installation}

\section{Requirements}
\label{sec.requirements}

For building and running \scalaris{}, some third-party software is
required which is not included in the \scalaris{} sources:

\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
\item Erlang R13B01 or newer
\item OpenSSL (required by Erlang's crypto module)
\item GNU-like Make and autoconf (not required on Windows)
\end{itemize}

To build the Java API (and its command-line client) the following
programs are also required:

\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
\item Java Development Kit 6
\item Apache Ant
\end{itemize}

Before building the Java API, make sure that \code{JAVA\_HOME} and
\code{ANT\_HOME} are set. \code{JAVA\_HOME} has to point to a JDK
installation, and \code{ANT\_HOME} has to point to an Ant installation.

To build the Python API (and its command-line client) the following
programs are also required:

\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
\item Python >= 2.6
\end{itemize}

\section{Download}

The sources can be obtained from
\url{http://code.google.com/p/scalaris}. RPM and DEB packages are available
from \url{http://download.opensuse.org/repositories/home:/scalaris/} for
various Linux distributions.

\subsection{Development Branch}

You find the latest development version in the svn repository:
\begin{lstlisting}[language={}]
# Non-members may check out a read-only working copy anonymously over HTTP.
svn checkout http://scalaris.googlecode.com/svn/trunk/ scalaris-read-only
\end{lstlisting}

\subsection{Releases}

Releases can be found under the 'Download' tab on the web-page.


\section{Build}

\subsection{Linux}

\scalaris{} uses autoconf for configuring the build environment and
GNU Make for building the code.

\begin{lstlisting}[language=sh]
%> ./configure
%> make
%> make docs
\end{lstlisting}

For more details read \code{README} in the main \scalaris{} checkout
directory.

\subsection{Windows}

We are currently not supporting \scalaris{} on Windows. However, we
have two small {\tt .bat} files for building and running \scalaris{}
nodes. It seems to work but we make no guarantees.

\begin{itemize}
\item Install Erlang\\
       \url{http://www.erlang.org/download.html}
\item Install OpenSSL (for crypto module)\\
       \url{http://www.slproweb.com/products/Win32OpenSSL.html}
\item Checkout \scalaris{} code from SVN
\item adapt the path to your Erlang installation in \code{build.bat}
\item start a \code{cmd.exe}
\item go to the \scalaris{} directory
\item run \code{build.bat} in the cmd window
\item check that there were no errors during the compilation;
       warnings are fine
\item go to the bin sub-directory
\item adapt the path to your Erlang installation in \code{firstnode.bat},
       \code{joining_node.bat}
\item run \code{firstnode.bat} or one of the other start scripts in the cmd window
\end{itemize}

\code{build.bat} will generate a \code{Emakefile} if there is none yet.
If you have Erlang $<$ R13B04, you will need to adapt the \code{Emakefile}.
There will be empty lines in the first three blocks ending with
``\code{ ]\}.}'': add the following to these lines and try to compile again.
It should work now.

\begin{lstlisting}[language=erlang]
, {d, type_forward_declarations_are_not_allowed}
, {d, forward_or_recursive_types_are_not_allowed}
\end{lstlisting}

For the most recent description please see the FAQ at
\url{http://code.google.com/p/scalaris/wiki/FAQ}.

\subsection{Java-API}

The following commands will build the Java API for \scalaris{}:
\begin{lstlisting}[language=sh]
%> make java
\end{lstlisting}

This will build {\tt scalaris.jar}, which is the library for accessing
the overlay network. Optionally, the documentation can be build:
\begin{lstlisting}[language=sh]
%> cd java-api
%> ant doc
\end{lstlisting}

\subsection{Python-API}

The Python API for Python 2.* (at least 2.6) is located in the \code{python-api}
directory. Files for Python 3.* can be created using \code{2to3} from the files
in \code{python-api}. The following command will use \code{2to3} to convert the
modules and place them in \code{python3-api}. 
\begin{lstlisting}[language=sh]
%> make python3
\end{lstlisting}
Both versions of python will compile required modules on demand when executing
the scripts for the first time. However, pre-compiled modules can be created
with:
\begin{lstlisting}[language=sh]
%> make python
%> make python3
\end{lstlisting}

\subsection{Ruby-API}

The Ruby API for Ruby >= 1.8 is located in the \code{ruby-api}
directory. Compilation is not necessary.

\section{Installation}
\label{sec:install}

For simple tests, you do not need to install \scalaris{}. You can run it
directly from the source directory. Note: \code{make install} will install
\scalaris{} into \code{/usr/local} and place \code{scalarisctl} into
\code{/usr/local/bin}, by default. But it is more convenient to build an RPM
and install it.

\begin{lstlisting}{language=sh}
svn checkout http://scalaris.googlecode.com/svn/trunk/ scalaris-0.0.1
tar -cvjf scalaris-0.0.1.tar.bz2 scalaris-0.0.1 --exclude-vcs
cp scalaris-0.0.1.tar.bz2 /usr/src/packages/SOURCES/
rpmbuild -ba scalaris-0.0.1/contrib/scalaris.spec
\end{lstlisting}

Your source and binary RPMs will be generated in
\code{/usr/src/packages/SRPMS} and \code{RPMS}. We build RPM and DEB packages
using checkouts from svn and provide them using the openSUSE BuildService at
\url{http://download.opensuse.org/repositories/home:/scalaris/}. Packages
are available for

\begin{itemize}
\item Fedora 12, 13, 14,
\item Mandriva 2009.1, 2010, 2010.1,
\item openSUSE 11.2, 11.3, 11.4, Factory, Tumbleweed
\item SLE 10, 11, 11SP1,
\item CentOS 5.5,
\item RHEL 5.5, 6,
\item Debian 5.0, 6.0 and
\item Ubuntu 9.04, 9.10, 10.04, 10.10.
\end{itemize}

An up-to-date list of available repositories can be found at
\url{https://code.google.com/p/scalaris/wiki/FAQ#Prebuild_packages}.

Inside those repositories you will also find an Erlang package - you don't
need this if you already have a recent enough Erlang version
(ref. Section~\sieheref{sec.requirements})!


\chapter{Setting up \scalaris{}}
\label{chapter.runscalaris}
\svnrev{r1810}

\section{Runtime Configuration}

\scalaris{} reads two configuration files from the working directory:
\code{bin/scalaris.cfg} (mandatory) and \code{bin/scalaris.local.cfg}
(optional). The former defines default settings and is included in the
release. The latter can be created by the user to alter settings.  A
sample file is provided as \code{bin/scalaris.local.cfg.example}. To run
\scalaris{} distributed over several nodes, each node requires a
\code{bin/scalaris.local.cfg}:

\codefile{scalaris.local.cfg}{../bin/scalaris.local.cfg.example}

A \scalaris{} deployment can have a management server and several nodes. The
management-server is optional and provides a global view on all nodes of a
\scalaris{} deployment which contact this server, i.e. have its address
specified in the \code{mgmt_server} configuration setting.

In this example, the \code{mgmt_server}'s location is defined as
an IP address plus a TCP port and its Erlang-internal process name.
If the deployment should not use a management server, replace the setting with
an invalid address, e.g. \code{'null'}.

\subsection{Logging}
\label{sec:logging}

\scalaris{} uses the log4erl library (see \code{contrib/log4erl}) for
logging status information and error messages. The log level can be
configured in \code{bin/scalaris.cfg} for both the stdout and file logger.
The default value is {\tt warn}; only warnings, errors and severe problems are
logged.

\begin{lstlisting}[language=erlang]
%% @doc Loglevel: debug < info < warn < error < fatal < none
{log_level, warn}.
{log_level_file, warn}.
\end{lstlisting}

In some cases, it might be necessary to get more complete logging
information, e.g. for debugging. In Chapter~\sieheref{chapter.join},
we are explaining the startup process of \scalaris{} nodes in more
detail, here the {\tt info} level provides more detailed information.

\begin{lstlisting}[language=erlang]
%% @doc Loglevel: debug < info < warn < error < fatal < none
{log_level, info}.
{log_level_file, info}.
\end{lstlisting}


\section{Running \scalaris{}}

As mentioned above, \scalaris{} consists of:
\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
\item management servers and
\item regular nodes
\end{itemize}

The management server will maintain a list of nodes participating in the
system. A regular node is either the first node in a system or joins an
existing system deployment.

\subsection{Running on a local machine}
\label{sec.boot}

Open at least two shells. In the first, inside the \scalaris{} directory,
start the first node (\code{firstnode.bat} on Windows):
\begin{lstlisting}[language=sh]
%> ./bin/firstnode.sh
\end{lstlisting}

This will start a new \scalaris{} deployment with a single node, including a
management server. On success \url{http://localhost:8000} should point to
the management interface page of the management server. The main page will
show you the number of nodes currently in the system.
A first \scalaris{} node should have started and the number should
show 1 node. The main page will also allow you to store and retrieve
key-value pairs but should not be used by applications to access
\scalaris{}. See Section~\sieheref{chapter.systemuse.apis} for application APIs.

In a second shell, you can now start a second \scalaris{} node. This
will be a `regular node':
\begin{lstlisting}[language=sh]
%> ./bin/joining_node.sh
\end{lstlisting}

The second node will read the configuration file and use this information to
contact a number of known nodes (set by the \code{known_hosts} configuration
setting) and join the ring. It will also register itself with the management
server.
The number of nodes on the web page should have increased to two by now.

Optionally, a third and fourth node can be started on the same
machine. In a third shell:
\begin{lstlisting}[language=sh]
%> ./bin/joining_node.sh 2
\end{lstlisting}

In a fourth shell:
\begin{lstlisting}[language=sh]
%> ./bin/joining_node.sh 3
\end{lstlisting}

This will add two further nodes to the deployment. The
\code{./bin/joining_node.sh} script accepts a number as its parameter which
will be added to the started node's name, i.e. \code{1} will lead to a node
named \code{node1}.
The web pages at \url{http://localhost:8000} should show the additional nodes.

\subsection{Running distributed}

\scalaris{} can be installed on other machines in the same way as
described in Section~\sieheref{sec:install}. In the default configuration,
nodes will look for the management server on \code{127.0.0.1} on port 14195. You
should create a \code{scalaris.local.cfg} pointing to the node running
the management server. You should also add a list of known nodes.

\codesnippet{scalaris.local.cfg}{local_cfg:distributed}{../bin/scalaris.local.cfg.example}

If you are starting the management server using \code{firstnode.sh}, it will
listen on port 14195 and you have to change the port and the IP address in the
configuration file. Otherwise the other nodes will not find the management
server. Calling \code{./bin/joining_node.sh} on a remote machine will start the
node and automatically contact the configured management server.

%\subsection{Running on PlanetLab}

%\subsection{Replication Degree}

%\subsection{Routing Scheme}

\section{Custom startup using \code{scalarisctl}}

On linux you can also use the \code{scalarisctl} script to start a
management server and `regular' nodes directly.
\begin{lstlisting}[language=sh]
%> ./bin/scalarisctl -h
\end{lstlisting}
\lstinputlisting[language={}]{scalarisctl-h.out}

\chapter{Using the system}
\label{chapter.systemuse}
\svnrev{r1936}

\scalaris{} can be used with one of the provided command line interfaces or
by using one of the APIs in a custom program. The following sections will
describe the APIs in general, each API in more detail and the use of our
command line interfaces.

\section{Application Programming Interfaces (APIs)}
\label{chapter.systemuse.apis}

Currently we offer the following APIs:
\begin{itemize}
  \item an \emph{Erlang API} running on the node \scalaris{} is run\\
        (functions can be called using remote connections with distributed
        Erlang)
  \item a \emph{Java API} using Erlang's \code{JInterface} library\\
        (connections are established using distributed Erlang)
  \item a generic \emph{JSON API}\\
        (offered by an integrated HTTP server running on each \scalaris{} node)
  \item a \emph{Python API} for Python >= 2.6 using JSON to talk to \scalaris{}. 
  \item a \emph{Ruby API} for Ruby >= 1.8 using JSON to talk to \scalaris{}. 
\end{itemize}

Each API contains methods for accessing functions from the three layers
\scalaris{} is composed of.
Table~\ref{api.layers} shows the modules and classes of Erlang, Java, Python and Ruby
and their mapping to these layers. The appropriate JSON calls
are shown in Section~\sieheref{sec.api.json}.

Special care needs to be taken when trying to delete keys (no matter which API
is used). This can only be done outside the transaction layer and is thus not
absolutely safe. Refer to the following thread on the mailing list:
\url{http://groups.google.com/group/scalaris/browse_thread/thread/ff1d9237e218799}.

\begin{table}
  \centering
    \begin{tabular}{p{4cm}lll}
    \toprule
                      & Erlang                 & Java                         & Python / Ruby              \\
                      & \footnotesize{module}  & \footnotesize{class in \code{de.zib.scalaris}}%
                                                                              & \footnotesize{class in module \code{scalaris}} \\
    \midrule
    Transaction Layer & \code{api_tx}          & \code{Transaction},          & \code{Transaction},        \\
                      &                        & \code{TransactionSingleOp}   & \code{TransactionSingleOp} \\
    %\cmidrule(lr){2-4}
                      & \code{api_pubsub}      & \code{PubSub}                & \code{PubSub}              \\
    \cmidrule(lr){1-4}
    Replication Layer & \code{api_rdht}        & \code{ReplicatedDHT}         & \code{ReplicatedDHT}       \\
    \cmidrule(lr){1-4}
    P2P Layer         & \code{api_dht}         &                              &                            \\
    %\cmidrule(lr){2-4}
                      & \code{api_dht_raw}     &                              &                            \\
    \bottomrule
    \end{tabular}
    \caption{Layered API structure}
    \label{api.layers}
\end{table}

\subsection{Supported Types}

Different programming languages have different types. In order for our APIs
to be compatible with each other, only a subset of the available types is
officially supported.

\emph{Keys} are always strings. In order to avoid problems with different
encodings on different systems, we suggest to only use ASCII characters.

For \emph{values} we distinguish between \emph{native}, \emph{composite}
and \emph{custom} types.

\emph{Native} types are
\begin{itemize}
  \item boolean values
  \item integer numbers
  \item floating point numbers
  \item strings and
  \item binary objects (a number of bytes).
\end{itemize}

\emph{Composite} types are
\begin{itemize}
  \item lists of native types (except binary objects)
  \item JavaScript Object Notation (JSON)\footnote{see \url{http://json.org/}}
\end{itemize}

\emph{Custom} types include any Erlang term not covered by the previous types.
Special care needs to be taken using custom types as they may not be accessible
through every API or may be misinterpreted by an API. The use of them is
discouraged.

Table~\ref{api.supported_types} shows the mapping of supported types to the
language-specific types of each API.

\begin{table}
  \centering
  \begin{threeparttable}[b]
    \begin{tabular}{llllll}
    \toprule
               & Erlang            & Java                         & JSON                            & Python                    & Ruby \\
    \midrule
    boolean    & \code{boolean()}  & \code{bool}, \code{Boolean}  & \code{true}, \code{false}       & \code{True}, \code{False} & \code{true}, \code{false} \\
    integer    & \code{integer()}  & \code{int}, \code{Integer}   & \code{int}                      & \code{int}                & \code{Fixnum}, \\
               &                   & \code{long}, \code{Long}     &                                 &                           & \code{Bignum} \\
               &                   & \code{BigInteger}            &                                 &                           & \\
    float      & \code{float()}    & \code{double}, \code{Double} & \code{int frac}                 & \code{float}              & \code{Float} \\
               &                   &                              & \code{int exp}                  &                           & \\
               &                   &                              & \code{int frac exp}             &                           & \\
    string     & \code{string()}   & \code{String}                & \code{string}                   & \code{str}                & \code{String} \\
    binary     & \code{binary()}   & \code{byte[]}                & \code{string}                   & \code{bytearray}          & \code{String} \\
               &                   &                              & \footnotesize{(base64-encoded)} &                           & \\
    list(type) & \code{[type()]}   & \code{List<Object>}          & \code{array}                    & \code{list}               & \code{Array} \\
    JSON       & \code{json_obj()}\tnote{*} & \code{Map<String, Object>} & \code{object}            & \code{dict}               & \code{Hash}\\
    custom     & \code{any()}      & \code{OtpErlangObject}       & \emph{/}                        & \emph{/}                  & \emph{/} \\
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
      \item[*] ~\vspace{-1.5em}%
\begin{lstlisting}[language=erlang]
json_obj() :: {struct, [Key::atom() | string(), Value::json_val()]}
json_val() :: string() | number() | json_obj() | {array, [any()]} | true | false | null
\end{lstlisting}
    \end{tablenotes}
    \caption{Types supported by the \scalaris{} APIs}
    \label{api.supported_types}
  \end{threeparttable}
\end{table}

\subsection{JSON API}
\label{sec.api.json}

\scalaris{} supports a JSON API for transactions. To minimize the necessary
round trips between a client and \scalaris{}, it uses request lists, which
contain all requests that can be done in parallel. The request list is then
send to a \scalaris{} node with a POST message. The result contains
a list of the results of the requests and - in case of a transaction - a
TransLog. To add further
requests to the transaction, the TransLog and another list of requests may
be send to \scalaris{}. This process may be repeated as often as necessary.
To finish the transaction, the request list can contain a 'commit' request
as the last element, which triggers the validation phase of the transaction
processing.
Request lists are also supported for single read/write operations, i.e.
every single operation is committed on its own. 

The JSON-API can be accessed via the \scalaris{}-Web-Server running on port
8000 by default and the page \code{jsonrpc.yaws} (For example at:
\url{http://localhost:8000/jsonrpc.yaws}).
Requests are issued by sending a JSON object with header
\code{"Content-type"="application/json"} to this URL.
The result will then be returned as a JSON object with the same content type.
The following table shows how both objects look like:

\begin{tabular}{p{0.45\textwidth}cp{0.45\textwidth}}
\bf Request & & \bf Result \\
\begin{lstlisting}[language=java]
{
  "jsonrpc": "2.0",
  "method" : "<method>",
  "params" : [<params>],
  "id"     : <number>
}
\end{lstlisting}
& &
\begin{lstlisting}[language=java]
{
  "result" : <result_object>,
  "id"     : <number>
}
\end{lstlisting}
\end{tabular}

The \code{id} in the request can be an arbitrary number which identifies the
request and is returned in the result.
The following operations (shown as \code{<method>(<params>)}) are currently
supported (the given result is the \code{<result_object>} mentioned above):
\begin{itemize}
  \item \code{nop(Value)} - no operation, result:
\begin{lstlisting}[language=java]
"ok"
\end{lstlisting}
  \item[] \hspace{-1.7em}single read/write:
  \item \code{req_list_commit_each(<req_list_rw>)} - commit each request in the list, result:
\begin{lstlisting}[language=java]
{[{"status": "ok"} or {"status": "ok", "value": <json_value>} or
  {"status": "fail", "reason": "timeout" or "abort" or "not_found"}]}
\end{lstlisting}
  \item \code{read(<key>)} - read the value at \code{key}, result:
\begin{lstlisting}[language=java]
{"status": "ok", "value", <json_value>} or
{"status": "fail", "reason": "timeout" or "not_found"}
\end{lstlisting}
  \item \code{write(<key>, <json_value>)} - write \code{value} (inside \code{json_value}) to \code{key}, result:
\begin{lstlisting}[language=java]
{"status": "ok"} or
{"status": "fail", "reason": "timeout" or "abort"}
\end{lstlisting}
  \item \code{test_and_set(<key>, OldValue, NewValue)} - atomic test-and-set
  (write \code{NewValue} to \code{key} if the current value is \code{OldValue}
   - both values are \code{<json_value>}), result:
\begin{lstlisting}[language=java]
{"status": "ok"} or
{"status": "fail", "reason": "timeout" or "abort" or "not_found"} or
{"status": "fail", "reason": "key_changed", "value": <json_value>}
\end{lstlisting}
  \item[] \hspace{-1.7em}transactions:
  \item \code{req_list(<req_list>)} - process a list of requests, result:
\begin{lstlisting}[language=java]
{"tlog": <tlog>,
 "results": [{"status": "ok"} or {"status": "ok", "value": <json_value>} or
             {"status": "fail", "reason": "timeout" or "abort" or "not_found"}]}
\end{lstlisting}
  \item \code{req_list(<tlog>, <req_list>)} - process a list of requests with a previous translog, result:
\begin{lstlisting}[language=java]
{"tlog": <tlog>,
 "results": [{"status": "ok"} or {"status": "ok", "value": <json_value>} or
             {"status": "fail", "reason": "timeout" or "abort" or "not_found"}]}
\end{lstlisting}
  \item[] \hspace{-1.7em}replication layer functions:
  \item \code{delete(<key>)} - delete the value at \code{key}, default timeout 2s, result:
\begin{lstlisting}[language=java]
{"ok": <number>, "results": ["ok" or "locks_set" or "undef"]} or
{"failure": "timeout", "ok": <number>, "results": ["ok" or "locks_set" or "undef"]}
\end{lstlisting}
  \item \code{delete(<key>, Timeout)} - delete the value at \code{key} with a timeout of \code{Timeout} Milliseconds, result:
\begin{lstlisting}[language=java]
{"ok": <number>, "results": ["ok" or "locks_set" or "undef"]} or
{"failure": "timeout", "ok": <number>, "results": ["ok" or "locks_set" or "undef"]}
\end{lstlisting}
  \item[] \hspace{-1.7em}raw DHT functions:
  \item \code{range_read(From, To)} - read a range of (raw) keys, result:
\begin{lstlisting}[language=java]
{"status": "ok" or "timeout",
 "value": [{"key": <key>, "value": <json_value>, "version": <version>}]}
\end{lstlisting}
  \item[] \hspace{-1.7em}publish/subscribe:
  \item \code{publish(Topic, Content)} - publish \code{Content} to \code{Topic} (\code{<key>}), result:
\begin{lstlisting}[language=java]
{"status": "ok"}
\end{lstlisting}
  \item \code{subscribe(Topic, URL)} - subscribe \code{URL} to \code{Topic} (\code{<key>}), result:
\begin{lstlisting}[language=java]
{"status": "ok"} or
{"status": "fail", "reason": "timeout" or "abort"}
\end{lstlisting}
  \item \code{unsubscribe(Topic, URL)} - unsubscribe \code{URL} from \code{Topic} (\code{<key>}), result:
\begin{lstlisting}[language=java]
{"status": "ok"} or
{"status": "fail", "reason": "timeout" or "abort" or "not_found"}
\end{lstlisting}
  \item \code{get_subscribers(Topic)} - get subscribers of \code{Topic} (\code{<key>}), result:
\begin{lstlisting}[language=java]
[<urls>]
\end{lstlisting}
%  \item \code{notify(Topic, Value)} - process a notify which came from a publish, result:
% \begin{lstlisting}[language=java]
% "ok"
% \end{lstlisting}
\end{itemize}

Note:
\begin{lstlisting}[language=java]
<json_value> = {"type": "as_is" or "as_bin", "value": <value>}
<req_list_rw> = [{"read", <key>} | {"write", {<key>: <json_value>}}]
<req_list> = [{"read", <key>} | {"write", {<key>: <json_value>}} | {"commit", _}]
\end{lstlisting}
The \code{<value>} inside \code{<json_value>} is either a base64-encoded
string representing a binary object (type = \code{"as_bin"}) or the value
itself (type = \code{"as_is"}).

\subsubsection{JSON-Example}

The following example illustrates the message flow:

%note: use tables inside tables instead of multicol for easier listings handling
\begin{longtable}{p{0.97\textwidth}}
\begin{tabular}{p{0.43\textwidth}cp{0.43\textwidth}}
\bf Client & & \hfill\bf \scalaris{} node \\
\end{tabular} \\
%
% request:
\begin{tabular}{p{0.43\textwidth}cp{0.43\textwidth}}
Make a transaction, that sets two keys & $\to$ & \\
\end{tabular}\vspace{-1.5em} \\
%
\begin{tabular}{p{0.77\textwidth}p{0.14\textwidth}}
\vspace{-1.5em}%
\begin{lstlisting}[language=java]
{"jsonrpc": "2.0",
 "method": "req_list",
 "params": [
  [ { "write": { "keyA": {"type": "as_is", "value": "valueA"} } },
    { "write": { "keyB": {"type": "as_is", "value": "valueB"} } },
    { "commit": "" } ]
 ],
 "id": 0
}
\end{lstlisting}
& \\
\end{tabular}\vspace{-1em} \\
%
% result:
\begin{tabular}{p{0.43\textwidth}cp{0.43\textwidth}}
 & $\leftarrow$ & \hfill{}\scalaris{} sends results back \\
\end{tabular}\vspace{-1.5em} \\

\begin{tabular}{p{0.14\textwidth}p{0.77\textwidth}}
& 
\vspace{-1.5em}%
\begin{lstlisting}[language=java]
{"error": null,
 "result": {
  "results": [ {"status": "ok"}, {"status": "ok"}, {"status": "ok"} ],
  "tlog": <TLOG> // this is the translog for further operations!
 },
 "id": 0
}
\end{lstlisting} \\
\end{tabular}\vspace{-1em} \\
%
% request:
\begin{tabular}{p{0.43\textwidth}cp{0.43\textwidth}}
In a second transaction: Read the two keys & $\to$ & \\
\end{tabular}\vspace{-1.5em} \\
%
\begin{tabular}{p{0.77\textwidth}p{0.14\textwidth}}
\vspace{-1.5em}%
\begin{lstlisting}[language=java]
{"jsonrpc": "2.0",
 "method": "req_list",
 "params": [
  [ { "read": "keyA" },
    { "read": "keyB" } ]
 ],
 "id": 0
}
\end{lstlisting}
& \\
\end{tabular}\vspace{-1em} \\
%
% result:
\begin{tabular}{p{0.43\textwidth}cp{0.43\textwidth}}
 & $\leftarrow$ & \hfill{}\scalaris{} sends results back \\
\end{tabular}\vspace{-1.5em} \\

\begin{tabular}{p{0.14\textwidth}p{0.77\textwidth}}
& 
\vspace{-1.5em}%
\begin{lstlisting}[language=java]
{"error": null,
 "result": {
  "results": [
   { "status": "ok", "value": {"type": "as_is", "value": "valueA"} },
   { "status": "ok", "value": {"type": "as_is", "value": "valueB"} }
  ],
  "tlog": <TLOG>
 },
 "id": 0
}
\end{lstlisting} \\
\end{tabular} \\
%
% request:
\begin{tabular}{p{0.43\textwidth}cp{0.43\textwidth}}
Calculate something with the read values and make further requests, here a
write and the commit for the whole transaction. Also include the latest
translog we got from \scalaris{} (named \code{<TLOG>} here). & $\to$ & \\
\end{tabular}\vspace{-1.5em} \\
%
\begin{tabular}{p{0.77\textwidth}p{0.14\textwidth}}
\vspace{-1.5em}%
\begin{lstlisting}[language=java]
{"jsonrpc": "2.0",
 "method": "req_list",
 "params": [
  <TLOG>,
  [ { "write": { "keyA": {"type": "as_is", "value": "valueA2"} } },
    { "commit": "" } ]
 ],
 "id": 0
}
\end{lstlisting}
& \\
\end{tabular}\vspace{-1em} \\
%
% result:
\begin{tabular}{p{0.43\textwidth}cp{0.43\textwidth}}
 & $\leftarrow$ & \hfill{}\scalaris{} sends results back \\
\end{tabular}\vspace{-1.5em} \\

\begin{tabular}{p{0.14\textwidth}p{0.77\textwidth}}
& 
\vspace{-1.5em}%
\begin{lstlisting}[language=java]
{"error": null,
 "result": {
  "results": [ {"status": "ok"}, {"status": "ok"} ],
  "tlog": <TLOG>
 },
 "id": 0
}
\end{lstlisting} \\
\end{tabular} \\
\end{longtable}

Examples of how to use the JSON API are the Python and Ruby API which use JSON
to communicate with \scalaris{}.

%\subsection{Erlang}

\subsection{Java API}

The \code{scalaris.jar} provides a Java command line client as well as a
library for Java programs to access \scalaris{}. The library provides several
classes:

\begin{itemize}
\item \code{TransactionSingleOp} provides methods for reading and writing values.
\item \code{Transaction} provides methods for reading and writing values in transactions.
\item \code{PubSub} provides methods for a simple topic-based pub/sub implementation on top of \scalaris{}.
\item \code{ReplicatedDHT} provides low-level methods for accessing the replicated DHT of \scalaris{}.
\end{itemize}

For details regarding the API we refer the reader to the Javadoc:

\begin{lstlisting}[language=sh]
%> cd java-api
%> ant doc
%> firefox doc/index.html
\end{lstlisting}

\section{Command Line Interfaces}

\subsection{Java command line interface}

As mentioned above, the \code{scalaris.jar} file contains a small command line
interface client. For
convenience, we provide a wrapper script called \code{scalaris} which
sets up the Java environment:

\begin{lstlisting}[language={}]
%> ./java-api/scalaris --help
./java-api/scalaris [script options] [options]
Script Options:
  --help, -h             print this message and scalaris help
  --noconfig             suppress sourcing of config files in $HOME/.scalaris/
                         and ${prefix}/etc/scalaris/
  --execdebug            print scalaris exec line generated by this
                         launch script
  --noerl                do not ask erlang for its (local) host name
  
usage: scalaris [Options]
 -b,--minibench <runs> <benchmarks>   run selected mini benchmark(s)
                                      [1|...|9|all] (default: all
                                      benchmarks, 100 test runs)
 -d,--delete <key> <[timeout]>        delete an item (default timeout:
                                      2000ms)
                                      WARNING: This function can lead to
                                      inconsistent data (e.g. deleted
                                      items can re-appear). Also when
                                      re-creating an item the version
                                      before the delete can re-appear.
 -g,--getsubscribers <topic>          get subscribers of a topic
 -h,--help                            print this message
 -lh,--localhost                      gets the local host's name as known
                                      to Java (for debugging purposes)
 -p,--publish <topic> <message>       publish a new message for the given
                                      topic
 -r,--read <key>                      read an item
 -s,--subscribe <topic> <url>         subscribe to a topic
 -u,--unsubscribe <topic> <url>       unsubscribe from a topic
 -v,--verbose                         print verbose information, e.g. the
                                      properties read
 -w,--write <key> <value>             write an item
\end{lstlisting}

\code{read}, \code{write} and \code{delete} can be used to read, write
and delete from/to the overlay, respectively. \code{getsubscribers},
\code{publish}, and \code{subscribe} are the PubSub functions. The others
provide debugging and testing functionality.

\begin{lstlisting}[language=]
%> ./java-api/scalaris -write foo bar
write(foo, bar)
%> ./java-api/scalaris -read foo
read(foo) == bar
\end{lstlisting}

Per default, the \code{scalaris} script tries to connect to a management
server at \code{localhost}. You can change the node it connects to (and
further connection properties) by adapting the values defined in
\code{java-api/scalaris.properties}.

\subsection{Python command line interface}

\begin{lstlisting}[language={}]
%> ./python-api/scalaris_client.py --help
usage: ./python-api/scalaris_client.py [Options]
 -r,--read <key>                      read an item
 -w,--write <key> <value>             write an item
 -d,--delete <key> [<timeout>]        delete an item (default timeout:
                                      2000ms)
                                      WARNING: This function can lead to
                                      inconsistent data (e.g. deleted
                                      items can re-appear). Also when
                                      re-creating an item the version
                                      before the delete can re-appear.
 -p,--publish <topic> <message>       publish a new message for the given
                                      topic
 -s,--subscribe <topic> <url>         subscribe to a topic
 -g,--getsubscribers <topic>          get subscribers of a topic
 -u,--unsubscribe <topic> <url>       unsubscribe from a topic
 -h,--help                            print this message
 -b,--minibench <runs> <benchmarks>   run selected mini benchmark(s)
                                      [1|...|9|all] (default: all
                                      benchmarks, 100 test runs)
\end{lstlisting}

\subsection{Ruby command line interface}

\begin{lstlisting}[language={}]
%> ./ruby-api/scalaris_client.rb --help
Usage: scalaris_client [options]
    -r, --read KEY                   read key KEY
    -w, --write KEY,VALUE            write key KEY to VALUE
    -h, --help                       Show this message
\end{lstlisting}


\chapter{Testing the system}
\svnrev{r1618}

\section{Erlang unit tests}
There are some unit tests in the \code{test} directory which test \scalaris{}
itself (the Erlang code). You can call them
by running \code{make test} in the main directory. The results are stored
in a local \code{index.html} file. 

The tests are implemented with the \code{common-test} package from the
Erlang system. For running the tests we rely on \code{run\_test},
which is part of the \code{common-test} package, but (on erlang $<$ R14) is not
installed by default. \code{configure} will check whether \code{run\_test} is
available. If it is not installed, it will show a warning and a short
description of how to install the missing file.

Note: for the unit tests, we are setting up and shutting down several
overlay networks. During the shut down phase, the runtime environment
will print extensive error messages. These error messages do not
indicate that tests failed! Running the complete test suite takes
about 10-20 minutes, depending on your machine.

If the test suite is interrupted before finishing, the results may not have
been linked into the \code{index.html} file. They are however stored in the
\code{ct_run.ct@...} directory.

\section{Java unit tests}
The Java unit tests can be run by executing \code{make java-test} in the main
directory. This will start a \scalaris{} node with the default ports and test
all Java functions part of the Java API. A typical run will look like the
following:

\begin{lstlisting}[language={}]
%> make java-test
[...]
tools.test:
    [junit] Running de.zib.tools.PropertyLoaderTest
    [junit] Testsuite: de.zib.tools.PropertyLoaderTest
    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 0.113 sec
    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 0.113 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] Working Directory = <scalarisdir>/java-api/classes
    [junit] ------------- ---------------- ---------------
[...]
scalaris.test:
    [junit] Running de.zib.scalaris.ConnectionTest
    [junit] Testsuite: de.zib.scalaris.ConnectionTest
    [junit] Tests run: 7, Failures: 0, Errors: 0, Time elapsed: 0.366 sec
    [junit] Tests run: 7, Failures: 0, Errors: 0, Time elapsed: 0.366 sec
    [junit] 
    [junit] Running de.zib.scalaris.DefaultConnectionPolicyTest
    [junit] Testsuite: de.zib.scalaris.DefaultConnectionPolicyTest
    [junit] Tests run: 12, Failures: 0, Errors: 0, Time elapsed: 0.314 sec
    [junit] Tests run: 12, Failures: 0, Errors: 0, Time elapsed: 0.314 sec
    [junit] 
    [junit] Running de.zib.scalaris.PeerNodeTest
    [junit] Testsuite: de.zib.scalaris.PeerNodeTest
    [junit] Tests run: 5, Failures: 0, Errors: 0, Time elapsed: 0.077 sec
    [junit] Tests run: 5, Failures: 0, Errors: 0, Time elapsed: 0.077 sec
    [junit] 
    [junit] Running de.zib.scalaris.PubSubTest
    [junit] Testsuite: de.zib.scalaris.PubSubTest
    [junit] Tests run: 33, Failures: 0, Errors: 0, Time elapsed: 4.105 sec
    [junit] Tests run: 33, Failures: 0, Errors: 0, Time elapsed: 4.105 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] 2011-03-25 15:07:04.412:INFO::jetty-7.3.0.v20110203
    [junit] 2011-03-25 15:07:04.558:INFO::Started SelectChannelConnector@127.0.0.1:59235
    [junit] 2011-03-25 15:07:05.632:INFO::jetty-7.3.0.v20110203
    [junit] 2011-03-25 15:07:05.635:INFO::Started SelectChannelConnector@127.0.0.1:41335
    [junit] 2011-03-25 15:07:05.635:INFO::jetty-7.3.0.v20110203
    [junit] 2011-03-25 15:07:05.643:INFO::Started SelectChannelConnector@127.0.0.1:38552
    [junit] 2011-03-25 15:07:05.643:INFO::jetty-7.3.0.v20110203
    [junit] 2011-03-25 15:07:05.646:INFO::Started SelectChannelConnector@127.0.0.1:34704
    [junit] 2011-03-25 15:07:06.864:INFO::jetty-7.3.0.v20110203
    [junit] 2011-03-25 15:07:06.864:INFO::Started SelectChannelConnector@127.0.0.1:57898
    [junit] 2011-03-25 15:07:06.864:INFO::jetty-7.3.0.v20110203
    [junit] 2011-03-25 15:07:06.865:INFO::Started SelectChannelConnector@127.0.0.1:47949
    [junit] 2011-03-25 15:07:06.865:INFO::jetty-7.3.0.v20110203
    [junit] 2011-03-25 15:07:06.866:INFO::Started SelectChannelConnector@127.0.0.1:53886
    [junit] 2011-03-25 15:07:07.090:INFO::jetty-7.3.0.v20110203
    [junit] 2011-03-25 15:07:07.093:INFO::Started SelectChannelConnector@127.0.0.1:33141
    [junit] 2011-03-25 15:07:07.094:INFO::jetty-7.3.0.v20110203
    [junit] 2011-03-25 15:07:07.096:INFO::Started SelectChannelConnector@127.0.0.1:39119
    [junit] 2011-03-25 15:07:07.096:INFO::jetty-7.3.0.v20110203
    [junit] 2011-03-25 15:07:07.097:INFO::Started SelectChannelConnector@127.0.0.1:41603
    [junit] ------------- ---------------- ---------------
    [junit] Running de.zib.scalaris.ReplicatedDHTTest
    [junit] Testsuite: de.zib.scalaris.ReplicatedDHTTest
    [junit] Tests run: 6, Failures: 0, Errors: 0, Time elapsed: 0.732 sec
    [junit] Tests run: 6, Failures: 0, Errors: 0, Time elapsed: 0.732 sec
    [junit] 
    [junit] Running de.zib.scalaris.TransactionSingleOpTest
    [junit] Testsuite: de.zib.scalaris.TransactionSingleOpTest
    [junit] Tests run: 28, Failures: 0, Errors: 0, Time elapsed: 0.632 sec
    [junit] Tests run: 28, Failures: 0, Errors: 0, Time elapsed: 0.632 sec
    [junit] 
    [junit] Running de.zib.scalaris.TransactionTest
    [junit] Testsuite: de.zib.scalaris.TransactionTest
    [junit] Tests run: 18, Failures: 0, Errors: 0, Time elapsed: 0.782 sec
    [junit] Tests run: 18, Failures: 0, Errors: 0, Time elapsed: 0.782 sec
    [junit] 

test:

BUILD SUCCESSFUL
Total time: 10 seconds
'jtest_boot@csr-pc9.zib.de'
\end{lstlisting}

\section{Python unit tests}
The Python unit tests can be run by executing \code{make python-test} in the
main directory. This will start a \scalaris{} node with the default ports and test
all Python functions part of the Python API. A typical run will look like the
following:

\begin{lstlisting}[language={}]
%> make python-test
[...]
testDoubleClose (TransactionSingleOpTest.TestTransactionSingleOp) ... ok
testRead_NotConnected (TransactionSingleOpTest.TestTransactionSingleOp) ... ok
testRead_NotFound (TransactionSingleOpTest.TestTransactionSingleOp) ... ok
testTestAndSetList1 (TransactionSingleOpTest.TestTransactionSingleOp) ... ok
testTestAndSetList2 (TransactionSingleOpTest.TestTransactionSingleOp) ... ok
testTestAndSetList_NotConnected (TransactionSingleOpTest.TestTransactionSingleOp) ... ok
testTestAndSetList_NotFound (TransactionSingleOpTest.TestTransactionSingleOp) ... ok
testTestAndSetString1 (TransactionSingleOpTest.TestTransactionSingleOp) ... ok
testTestAndSetString2 (TransactionSingleOpTest.TestTransactionSingleOp) ... ok
testTestAndSetString_NotConnected (TransactionSingleOpTest.TestTransactionSingleOp) ... ok
testTestAndSetString_NotFound (TransactionSingleOpTest.TestTransactionSingleOp) ... ok
testTransactionSingleOp1 (TransactionSingleOpTest.TestTransactionSingleOp) ... ok
testTransactionSingleOp2 (TransactionSingleOpTest.TestTransactionSingleOp) ... ok
testWriteList1 (TransactionSingleOpTest.TestTransactionSingleOp) ... ok
testWriteList2 (TransactionSingleOpTest.TestTransactionSingleOp) ... ok
testWriteList_NotConnected (TransactionSingleOpTest.TestTransactionSingleOp) ... ok
testWriteString1 (TransactionSingleOpTest.TestTransactionSingleOp) ... ok
testWriteString2 (TransactionSingleOpTest.TestTransactionSingleOp) ... ok
testWriteString_NotConnected (TransactionSingleOpTest.TestTransactionSingleOp) ... ok
testAbort_Empty (TransactionTest.TestTransaction) ... ok
testAbort_NotConnected (TransactionTest.TestTransaction) ... ok
testCommit_Empty (TransactionTest.TestTransaction) ... ok
testCommit_NotConnected (TransactionTest.TestTransaction) ... ok
testDoubleClose (TransactionTest.TestTransaction) ... ok
testRead_NotConnected (TransactionTest.TestTransaction) ... ok
testRead_NotFound (TransactionTest.TestTransaction) ... ok
testTransaction1 (TransactionTest.TestTransaction) ... ok
testTransaction3 (TransactionTest.TestTransaction) ... ok
testWriteList1 (TransactionTest.TestTransaction) ... ok
testWriteString (TransactionTest.TestTransaction) ... ok
testWriteString_NotConnected (TransactionTest.TestTransaction) ... ok
testWriteString_NotFound (TransactionTest.TestTransaction) ... ok
testDelete1 (ReplicatedDHTTest.TestReplicatedDHT) ... ok
testDelete2 (ReplicatedDHTTest.TestReplicatedDHT) ... ok
testDelete_notExistingKey (ReplicatedDHTTest.TestReplicatedDHT) ... ok
testDoubleClose (ReplicatedDHTTest.TestReplicatedDHT) ... ok
testReplicatedDHT1 (ReplicatedDHTTest.TestReplicatedDHT) ... ok
testReplicatedDHT2 (ReplicatedDHTTest.TestReplicatedDHT) ... ok
testDoubleClose (PubSubTest.TestPubSub) ... ok
testGetSubscribersOtp_NotConnected (PubSubTest.TestPubSub) ... ok
testGetSubscribers_NotExistingTopic (PubSubTest.TestPubSub) ... ok
testPubSub1 (PubSubTest.TestPubSub) ... ok
testPubSub2 (PubSubTest.TestPubSub) ... ok
testPublish1 (PubSubTest.TestPubSub) ... ok
testPublish2 (PubSubTest.TestPubSub) ... ok
testPublish_NotConnected (PubSubTest.TestPubSub) ... ok
testSubscribe1 (PubSubTest.TestPubSub) ... ok
testSubscribe2 (PubSubTest.TestPubSub) ... ok
testSubscribe_NotConnected (PubSubTest.TestPubSub) ... ok
testSubscription1 (PubSubTest.TestPubSub) ... ok
testSubscription2 (PubSubTest.TestPubSub) ... ok
testSubscription3 (PubSubTest.TestPubSub) ... ok
testSubscription4 (PubSubTest.TestPubSub) ... ok
testUnsubscribe1 (PubSubTest.TestPubSub) ... ok
testUnsubscribe2 (PubSubTest.TestPubSub) ... ok
testUnsubscribe_NotConnected (PubSubTest.TestPubSub) ... ok
testUnsubscribe_NotExistingTopic (PubSubTest.TestPubSub) ... ok
testUnsubscribe_NotExistingUrl (PubSubTest.TestPubSub) ... ok

----------------------------------------------------------------------
Ran 58 tests in 12.317s

OK
'jtest_boot@csr-pc9.zib.de'
\end{lstlisting}

\section{Interoperability Tests}
In order to check whether the common types described in
Section~\sieheref{chapter.systemuse.apis} are fully supported by the APIs
and yield to the appropriate types in another API, we implemented some
interoperability tests. They can be run by executing \code{make interop-test}
in the main directory.
This will start a \scalaris{} node with the default ports, write test data using
both the Java and the Python APIs and let each API read the data it wrote
itself as well as the data the other API read. On success it will print

\begin{lstlisting}[language={}]
%> make interop-test
[...]
all tests successful
\end{lstlisting}

\chapter{Troubleshooting}
\svnrev{r1618}

\section{Network}

\scalaris{} uses a couple of TCP ports for communication. It does
not use UDP at the moment.

\begin{center}
\begin{tabular}{lll}
\toprule
 & HTTP Server & Inter-node communication \\
\midrule
default (see \code{bin/scalaris.cfg})                 & $8000$    & $14195$--$14198$ \\
first node (\code{bin/firstnode.sh})                  & $8000$    & $14195$          \\
joining node 1 (\code{bin/joining_node.sh})           & $8001$    & $14196$          \\
other joining nodes (\code{bin/joining_node.sh <ID>}) & $8000+\texttt{<ID>}$ & $14195+\texttt{<ID>}$ \\
standalone mgmt server (\code{bin/mgmt-server.sh})    & $7999$    & $14194$          \\
\bottomrule
\end{tabular}
\end{center}

Please make sure that at least 14195 and 14196 are not blocked by
firewalls in order to be able to start at least one first and one joining node
on each machine..

\section{Miscellaneous}
For up-to-date information about frequently asked questions and
troubleshooting, please refer to our FAQs at
\url{https://code.google.com/p/scalaris/wiki/FAQ} and our mailing list at
\url{http://groups.google.com/group/scalaris}.


\part{Developers Guide}

\chapter{General Hints}

\section{Coding Guidelines}

\begin{itemize}
\item Keep the code short
\item Use \erlmodule{gen\_component} to implement additional processes
\item Don't use receive by yourself (Exception: to implement single threaded
  user API calls (cs\_api, yaws\_calls, etc)
\item Don't use \erlfun{erlang}{now}{/0}, \erlfun{erlang}{send\_after}{/3},
  \code{receive after} etc. in performance critical code, consider
  using \erlmodule{msg\_delay} instead.
\item Don't use \erlfun{timer}{tc}{/3} as it catches exceptions. Use
  \erlfun{util}{tc}{/3} instead.
\end{itemize}

\section{Testing Your Modifications and Extensions}

\begin{itemize}
\item Run the testsuites using \code{make test}
\item Run the java api test using \code{make java-test}
      (\scalaris{} output will be printed if a test fails; if you want to see
      it during the tests, start a \code{bin/firstnode.sh} and run the tests by
      \code{cd java; ant test})
\item Run the Ruby client by starting \scalaris{} and running
      \code{cd contrib; ./jsonrpc.rb}
\end{itemize}

\section{Help with Digging into the System}
\label{sec:digging}

\begin{itemize}
\item use \erlfun{ets}{i}{/0,1} to get details on the local state of some
      processes
\item consider changing pdb.erl to use ets instead of erlang:put/get
\item Have a look at strace -f -p PID of beam process
\item Get message statistics via the Web-interface
\item enable/disable tracing for certain modules
%% \item enable gen-component profiling and read the collected data using
%% 
%%    \code{lists:reverse(lists:keysort(2,ets:tab2list(profiling))).}
%% 
%%    (Has the limitation that it measures using absolute time, maybe the data
%%    is more ok, when using -smp disable? ...)
\item Use etop and look at the total memory size and atoms generated
\item send processes sleep or kill messages to test certain behaviour (see
  \erlmodule{gen\_component}.erl)
\item use \code{mgmt_server:number_of_nodes(). flush().}
\item use \code{admin_checkring(). flush().}
\end{itemize}

%% \section{General Erlang server loop}
%% 
%% Servers in Erlang often use the following structure to maintain a state
%% while processing received messages:
%% 
%% \lstset{language=erlang}
%% \begin{lstlisting}
%% loop(State) ->
%%   receive
%%     Message ->
%%       State1 = f(State),
%%       loop(State1)
%%   end.
%% \end{lstlisting}
%% 
%% The server runs an endless loop, that waits for a message, processes it and
%% calls itself using tail-recursion in each branch. The loop works on a
%% \code{State}, which can be modified when a message is handled.

\chapter{System Infrastructure}

\section{Groups of Processes}
\label{sec:pid_groups}
\erlmoduleindex{pid\_groups}

\begin{itemize}
\item What is it? How to distinguish from Erlangs internal named processes?
\item Joining a process group
\item Why do we do this... (managing several independent nodes inside a single
  Erlang VM for testing)
\end{itemize}

\section{\texorpdfstring{The Communication Layer \erlmodule{comm}}
          {The Communication Layer comm}}
\label{sec:comm}
\erlmoduleindex{comm}

\begin{itemize}
\item in general
\item format of messages (tuples)
\item use messages with cookies (server and client side)
\item What is a message tag?
\end{itemize}

\input{gen_component}

\section{The Process' Database (\texttt{pdb})}
\erlmoduleindex{pdb}

\begin{itemize}
\item How to use it and how to switch from erlang:put/set to ets and implied
  limitations.
\end{itemize}

\section{Failure Detectors (\texttt{fd})}
\erlmoduleindex{fd}

\begin{itemize}
\item uses Erlang monitors locally
\item is independent of component load
\item uses heartbeats between Erlang virtual machines
\item uses a single proxy heartbeat server per Erlang virtual machine, which
  itself uses Erlang monitors to monitor locally
\item uses dynamic timeouts to implement an eventually perfect failure detector.
\end{itemize}

\section{Writing Unittests}

\subsection{Plain unittests}

\subsection{Randomized Testing using \texttt{tester.erl}}


\chapter{Basic Structured Overlay}

\section{Ring Maintenance}

\section{T-Man}

\section{Routing Tables}
\label{chapter.routing}
\svnrev{r1453}
\erlmoduleindex{rt\_beh}

Each node of the ring can perform searches in the overlay.

A search is done by a lookup in the overlay, but there are several
other demands for communication between peers. \scalaris{} provides
a general interface to route a message to the (other) peer, which is
currently responsible for a given \code{key}.

\codesnippet{api_dht_raw.erl}{api_dht_raw:lookup}{../src/api_dht_raw.erl}

The message \code{Msg} could be a \code{get_key} which retrieves content from
the responsible node or a \code{get_node} message, which returns a pointer
to the node.

All currently supported messages are listed in the file \code{dht_node.erl}.

The message routing is implemented in \code{dht_node_lookup.erl}

\codesnippet{dht_node_lookup.erl}{dht_node_lookup:routing}{../src/dht_node_lookup.erl}

Each node is responsible for a certain key interval. The function
\erlfun{intervals}{in}{/2} is used to decide, whether the key is between
the current node and its successor. If that is the case, the final step is
delivers a \code{lookup_fin} message to the local node. Otherwise, the message
is forwarded to the next nearest known peer (listed in the routing table)
determined by \erlfun{?RT}{next\_hop}{/2}.

\code{rt_beh.erl} is a generic interface for routing tables. It
can be compared to interfaces in Java. In Erlang interfaces can be
defined using a so called `behaviour'.  The files \code{rt_simple} and
\code{rt_chord} implement the behaviour `rt\_beh'.

The macro \code{?RT} is used to select the current implementation of routing
tables. It is defined in \code{include/scalaris.hrl}.

\codesnippet{scalaris.hrl}{scalaris:rt}{../include/scalaris.hrl}

The functions, that have to be implemented for a routing mechanism are
defined in the following file:

\codesnippet{rt_beh.erl}{rt_beh:behaviour}{../src/rt_beh.erl}

\begin{description}
\setlength{\parskip}{0pt}
\setlength{\itemsep}{0pt}
\erlfunindex{rt\_beh}{empty}
\item \code{empty/1} gets a successor and generates an empty routing
  table for use inside the routing table implementation. The data structure of
  the routing table is undefined. It can be a list, a tree, a matrix \ldots

\erlfunindex{rt\_beh}{empty\_ext}
\item \code{empty_ext/1} similarly creates an empty external routing table
  for use by the \erlmodule{dht\_node}. This process might not need all the
  information a routing table implementation requires and can thus work with
  less data.

\erlfunindex{rt\_beh}{hash\_key}
\item \code{hash_key/1} gets a key and maps it into the overlay's
  identifier space.

\erlfunindex{rt\_beh}{get\_random\_node\_id}
\item \code{get_random_node_id/0} returns a random node id from the
  overlay's identifier space. This is used for example when a new node
  joins the system.

\erlfunindex{rt\_beh}{next\_hop}
\item \code{next_hop/2} gets a \erlmodule{dht\_node}'s state (including the
  external routing table representation) and a key and returns the node, that
  should be contacted next when searching for the key, i.e. the known node
  nearest to the id.

\erlfunindex{rt\_beh}{init\_stabilize}
\item \code{init_stabilize/2} is called periodically to rebuild the
  routing table. The parameters are the identifier of the node, its
  successor and the old (internal) routing table state. This method may send
  messages to the \code{routing_table} process which need to be handled by
  the \code{handle_custom_message/2} handler since they are
  implementation-specific.

\erlfunindex{rt\_beh}{update}
\item \code{update/7} is called when the node's ID, predecessor and/or
  successor changes. It updates the (internal) routing table with the (new)
  information.

\erlfunindex{rt\_beh}{filter\_dead\_node}
\item \code{filter_dead_node/2} is called by the failure detector and tells
  the routing table about dead nodes. This function gets the (internal) routing
  table and a node to remove from it. A new routing table state is returned.

\erlfunindex{rt\_beh}{to\_pid\_list}
\item \code{to_pid_list/1} get the PIDs of all (internal) routing table
  entries.

\erlfunindex{rt\_beh}{get\_size}
\item \code{get_size/1} get the (internal or external) routing table's size.

\erlfunindex{rt\_beh}{get\_replica\_keys}
\item \code{get_replica_keys/1} Returns for a given (hashed)
  \code{Key} the (hashed) keys of its replicas. This used for implementing
  symmetric replication.

\erlfunindex{rt\_beh}{n}
\item \code{n/0} gets the number of available keys. An implementation may
  throw \code{throw:not_supported} if the operation is unsupported by
  the routing table.

\erlfunindex{rt\_beh}{dump}
\item \code{dump/1} dump the (internal) routing table state for debugging,
  e.g. by using the web interface. Returns a list of
  \code{\{Index, Node_as_String\}} tuples which may just as well be empty.

\erlfunindex{rt\_beh}{to\_list}
\item \code{to_list/1} convert the (external) representation of the routing
  table inside a given \code{dht_node_state} to a sorted list of known nodes
  from the routing table, i.e. first=succ, second=next known node on the ring,
  \ldots This is used by bulk-operations to create a
  broadcast tree.

\erlfunindex{rt\_beh}{export\_rt\_to\_dht\_node}
\item \code{export_rt_to_dht_node/2} convert the internal routing table state
  to an external state. Gets the internal state and the node's neighborhood
  for doing so.

\erlfunindex{rt\_beh}{handle\_custom\_message}
\item \code{handle_custom_message/2} handle messages specific to the routing
  table implementation. \erlmodule{rt\_loop} will forward unknown messages
  to this function.

\erlfunindex{rt\_beh}{check}
\item \code{check/5}, \code{check/6} check for routing table changes and send
  an updated (external) routing table to the \erlmodule{dht\_node} process.

\erlfunindex{rt\_beh}{check\_config}
\item \code{check_config/0} check that all required configuration parameters
  exist and satisfy certain restrictions.

\end{description}

\subsection{The routing table process (\texorpdfstring{\code{rt_loop}}{rt\_loop})}
\erlmoduleindex{rt\_loop}

The \erlmodule{rt\_loop} module implements the process for all routing tables.
It processes messages and calls the appropriate methods in the specific routing
table implementations.

\codesnippet{rt_loop.erl}{rt_loop:state}{../src/rt_loop.erl}
If initialized, the node's id, its predecessor, successor and the routing table
state of the selected implementation (the macro \code{RT} refers to).

\codesnippet{rt_loop.erl}{rt_loop:trigger}{../src/rt_loop.erl}
Periodically (see \code{routingtable_trigger} and
\code{pointer_base_stabilization_interval} config parameters) a \code{trigger}
message is sent to the \code{rt_loop} process that starts the periodic
stabilization implemented by each routing table.

\codesnippet{rt_loop.erl}{rt_loop:update_rt}{../src/rt_loop.erl}
Every time a node's neighborhood changes, the \erlmodule{dht\_node} sends an
\code{update_rt} message to the routing table which will call
\erlfun{?RT}{update}{/7} that decides whether the routing table should be
re-build. If so, it will stop any waiting trigger and schedule an immideate
(periodic) stabilization.

\subsection{Simple routing table (\texorpdfstring{\code{rt_simple}}{rt\_simple})}
\erlmoduleindex{rt\_simple}

One implementation of a routing table is the \code{rt_simple}, which routes
via the successor. Note that this is inefficient as it needs a linear number
of hops to reach its goal. A more robust implementation, would use a successor
list. This implementation is also not very efficient in the presence of churn.

\subsubsection{Data types}
First, the data structure of the routing table is defined:

\codesnippet{rt_simple.erl}{rt_simple:types}{../src/rt_simple.erl}
The routing table only consists of a node (the successor). Keys in the overlay
are identified by integers $\geq 0$.

\subsubsection{A simple \texorpdfstring{\erlmodule{rm\_beh}}{rm\_beh} behaviour}

\erlfunindex{rt\_simple}{empty}
\codesnippet{rt_simple.erl}{rt_simple:empty}{../src/rt_simple.erl}
\erlfunindex{rt\_simple}{empty\_ext}
\codesnippet{rt_simple.erl}{rt_simple:empty_ext}{../src/rt_simple.erl}
The empty routing table (internal or external)  consists of the successor.

\erlfunindex{rt\_simple}{hash\_key}
\codesnippet{rt_simple.erl}{rt_simple:hash_key}{../src/rt_simple.erl}
Keys are hashed using MD5 and have a length of 128 bits.

\erlfunindex{rt\_simple}{get\_random\_node\_id}
\codesnippet{rt_simple.erl}{rt_simple:get_random_node_id}{../src/rt_simple.erl}
Random node id generation uses the helpers provided by the \erlmodule{randoms}
module.

\erlfunindex{rt\_simple}{next\_hop}
\codesnippet{rt_simple.erl}{rt_simple:next_hop}{../src/rt_simple.erl}
Next hop is always the successor.

\erlfunindex{rt\_simple}{init\_stabilize}
\codesnippet{rt_simple.erl}{rt_simple:init_stabilize}{../src/rt_simple.erl}
\code{init_stabilize/2} resets its routing table to the current successor.

\erlfunindex{rt\_simple}{update}
\codesnippet{rt_simple.erl}{rt_simple:update}{../src/rt_simple.erl}
\code{update/7} updates the routing table with the new successor.

\erlfunindex{rt\_simple}{filter\_dead\_node}
\codesnippet{rt_simple.erl}{rt_simple:filter_dead_node}{../src/rt_simple.erl}
\code{filter_dead_node/2} does nothing, as only the successor is listed in
the routing table and that is reset periodically in \code{init_stabilize/2}.

\erlfunindex{rt\_simple}{to\_pid\_list}
\codesnippet{rt_simple.erl}{rt_simple:to_pid_list}{../src/rt_simple.erl}
\code{to_pid_list/1} returns the pid of the successor.

\erlfunindex{rt\_simple}{get\_size}
\codesnippet{rt_simple.erl}{rt_simple:get_size}{../src/rt_simple.erl}
The size of the routing table is always \code{1}.

\erlfunindex{rt\_simple}{get\_replica\_keys}
\codesnippet{rt_simple.erl}{rt_simple:get_replica_keys}{../src/rt_simple.erl}
This \code{get_replica_keys/1} implements symmetric replication.

\erlfunindex{rt\_simple}{n}
\codesnippet{rt_simple.erl}{rt_simple:n}{../src/rt_simple.erl}
There are $2^{128}$ available keys.

\erlfunindex{rt\_simple}{dump}
\codesnippet{rt_simple.erl}{rt_simple:dump}{../src/rt_simple.erl}
\code{dump/1} lists the successor.

\erlfunindex{rt\_simple}{to\_list}
\codesnippet{rt_simple.erl}{rt_simple:to_list}{../src/rt_simple.erl}
\code{to_list/1} lists the successor from the external routing table state.

\erlfunindex{rt\_simple}{export\_rt\_to\_dht\_node}
\codesnippet{rt_simple.erl}{rt_simple:export_rt_to_dht_node}{../src/rt_simple.erl}
\code{export_rt_to_dht_node/2} states that the external routing table is the
same as the internal table.

\erlfunindex{rt\_simple}{handle\_custom\_message}
\codesnippet{rt_simple.erl}{rt_simple:handle_custom_message}{../src/rt_simple.erl}
Custom messages could be send from a routing table process on one node to the
routing table process on another node and are independent from any other
implementation.

\codesnippet{rt_simple.hrl}{rt_simple:check}{../src/rt_simple.erl}
Checks whether the routing table changed and in this case sends the
\erlmodule{dht\_node} an updated (external) routing table state. Optionally
the failure detector is updated. This may not be necessary, e.g. if
\code{check} is called after a crashed node has been reported by the failure
detector (the failure detector already unsubscribes the node in this case).

\subsection{Chord routing table (\texorpdfstring{\code{rt_chord}}{rt\_chord})}
\erlmoduleindex{rt\_chord}

The file \code{rt_chord.erl} implements Chord's routing.

\subsubsection{Data types}

\codesnippet{rt_chord.erl}{rt_chord:types}{../src/rt_chord.erl}

The routing table is a \code{gb_tree}. Identifiers in the ring are
integers. Note that in Erlang integer can be of arbitrary
precision. For Chord, the identifiers are in $[0, 2^{128})$,
i.e. 128-bit strings.

\subsubsection{The \texorpdfstring{\erlmodule{rm\_beh}}{rm\_beh} behaviour for Chord (excerpt)}

\erlfunindex{rt\_chord}{empty}
\codesnippet{rt_chord.erl}{rt_chord:empty}{../src/rt_chord.erl}
\erlfunindex{rt\_chord}{empty\_ext}
\codesnippet{rt_chord.erl}{rt_chord:empty_ext}{../src/rt_chord.erl}
\code{empty/1} returns an empty \code{gb_tree}, same for \code{empty_ext/1}.

\erlfun{rt\_chord}{hash\_key}{/1},
\erlfun{rt\_chord}{get\_random\_node\_id}{/0},
\erlfun{rt\_chord}{get\_replica\_keys}{/1} and
\erlfun{rt\_chord}{n}{/0} are implemented like their counterparts in
\code{rt_simple.erl}.

\erlfunindex{rt\_chord}{next\_hop}
\codesnippet{rt_chord.erl}{rt_chord:next_hop}{../src/rt_chord.erl}
If the (external) routing table contains at least one item, the next hop is
retrieved from the \code{gb_tree}. It will be the node with the largest id
that is smaller than the id we are looking for. If the routing table is empty,
the successor is chosen. However, if we haven't found the key in our routing
table, the next hop will be our largest finger, i.e. entry.

\erlfunindex{rt\_chord}{init\_stabilize}
\codesnippet{rt_chord.erl}{rt_chord:init_stabilize}{../src/rt_chord.erl}
The routing table stabilization is triggered for the first index and
then runs asynchronously, as we do not want to block the
\code{rt_loop} to perform other request while recalculating the
routing table.

We have to find the node responsible for the calculated finger and therefore
perform a lookup for the node with a \code{rt_get_node} message, including
a reference to ourselves as the reply-to address and the index to be set.

The lookup performs an overlay routing by passing the message until
the responsible node is found. There, the message is delivered to the
\erlmodule{routing\_table} process
The remote node sends the requested information back directly. It includes a
reference to itself in a \code{rt_get_node_response} message. Both messages
are handled by \erlfun{rt\_chord}{handle\_custom\_message}{/2}:

\erlfunindex{rt\_chord}{handle\_custom\_message}
\codesnippet{rt_chord.erl}{rt_chord:handle_custom_message}{../src/rt_chord.erl}
\erlfunindex{rt\_chord}{stabilize}
\codesnippet{rt_chord.erl}{rt_chord:stabilize}{../src/rt_chord.erl}

\code{stabilize/5} assigns the received routing table entry and triggers the
routing table stabilization for the the next shorter entry using the same
mechanisms as described above.

If the shortest finger is the successor, then filling the routing table is
stopped, as no further new entries would occur. It is not necessary, that
\code{Index} reaches 1 to make that happen. If less than $2^{128}$ nodes
participate in the system, it may happen earlier.

\erlfunindex{rt\_chord}{update}
\codesnippet{rt_chord.erl}{rt_chord:update}{../src/rt_chord.erl}
Tells the \erlmodule{rt\_loop} process to rebuild the routing table starting
with an empty (internal) routing table state.

\erlfunindex{rt\_chord}{filter\_dead\_node}
\codesnippet{rt_chord.erl}{rt_chord:filter_dead_node}{../src/rt_chord.erl}
\code{filter_dead_node} removes dead entries from the \code{gb_tree}.

\erlfunindex{rt\_chord}{export\_rt\_to\_dht\_node}
\codesnippet{rt_chord.erl}{rt_chord:export_rt_to_dht_node}{../src/rt_chord.erl}
\code{export_rt_to_dht_node} converts the internal \code{gb_tree} structure
based on indices into the external representation optimised for look-ups, i.e.
a \code{gb_tree} with node ids and the nodes themselves.

\codesnippet{rt_chord.hrl}{rt_chord:check}{../src/rt_chord.erl}
Checks whether the routing table changed and in this case sends the
\erlmodule{dht\_node} an updated (external) routing table state. Optionally
the failure detector is updated. This may not be necessary, e.g. if
\code{check} is called after a crashed node has been reported by the failure
detector (the failure detector already unsubscribes the node in this case).


\section{Local Datastore}

\section{Cyclon}

\section{Vivaldi Coordinates}

\section{Estimated Global Information (Gossiping)}

\section{Load Balancing}

\section{Broadcast Trees}



\chapter{Transactions in \scalaris{}}
\label{chapter.transactions}

\section{The Paxos Module}

\section{Transactions using Paxos Commit}

\section{Applying the Tx-Modules to replicated DHTs}

Introduces transaction processing on top of a Overlay



\chapter{How a node joins the system}
\label{chapter.join}
\svnrev{r1370}

After starting a new \scalaris{}-System as described in
Section~\sieheref{sec.boot}, ten additional local nodes can be started
by typing \code{admin:add_nodes(10)} in the Erlang-Shell that the management
server opened~\footnote{Increase the log level to {\tt info} to get
more detailed startup logs. See Section~\sieheref{sec:logging}}.


\lstset{language=erlang}
\begin{lstlisting}
scalaris> ./bin/firstnode.sh 
[...]
(firstnode@csr-pc9)1> admin:add_nodes(10)
\end{lstlisting}

In the following we will trace what this function does in order to add
additional nodes to the system.
The function \code{admin:add_nodes(pos_integer())} is defined as follows.

\codesnippet{admin.erl}{admin:add_nodes}{../src/admin.erl}

It calls \erlfun{admin}{add\_node}{[]} Count times. This function starts a new
child with the given options for the main supervisor \code{main_sup}.
In particular, it sets a random ID that is passed to the new node as its
suggested ID to join at.
To actually perform the start, the
function \erlfun{sup\_dht\_node}{start\_link}{/1} is called by the Erlang
supervisor mechanism. For more details on the OTP supervisor
mechanism see Chapter~18 of the Erlang book~\cite{erlang-book} or the
online documentation at
\url{http://www.erlang.org/doc/man/supervisor.html}.

\section{Supervisor-tree of a \scalaris{} node}

When a new Erlang VM with a \scalaris{} node is started, a
\erlmodule{sup\_scalaris} supervisor is started that creates further
workers and supervisors according to the following scheme
(processes starting order: left to right, top to bottom):

\begin{center}
\includegraphics[width=\linewidth]{supervision}
\end{center}

When new nodes are started using \erlfun{admin}{add\_node}{/1}, only new
\code{sup_dht_node} supervisors are started.

\section{Starting the sup\_dht\_node supervisor and general processes of a node}

Starting supervisors is a two step process: a call to
\erlfun{supervisor}{start\_link}{/2,3}, e.g. from a custom supervisor's own
\code{start_link} method, will start the supervisor process. It will then
call \erlfun[]{Module}{init}{/1} to find out about the restart strategy,
maximum restart frequency and child processes.
Note that \erlfun{supervisor}{start\_link}{/2,3} will not return until
\erlfun[]{Module}{init}{/1} has returned and all child processes have been
started.

Let's have a look at \erlfun{sup\_dht\_node}{init}{/1}, the 'DHT node
supervisor'.

\codesnippet{sup_dht_node.erl}{sup_dht_node:init}{../src/sup_dht_node.erl}


The return value of the \code{init/1} function specifies the child
processes of the supervisor and how to start them. Here, we define a
list of processes to be observed by a \code{one_for_one}
supervisor. The processes are:
\code{Monitor},
\code{Delayer},
\code{Reregister},
\code{DeadNodeCache},
\code{RingMaintenance},
\code{RoutingTable},
\code{Cyclon},
\code{Vivaldi},
\code{DC_Clustering},
\code{Gossip} and a
\code{SupDHTNodeCore_AND} process in this order.

The term \code{\{one_for_one, 10, 1\}} specifies that the supervisor
should try 10 times to restart each process before giving
up. \code{one_for_one} supervision means, that if a single process
stops, only that process is restarted. The other processes run
independently.

When the \erlfun{sup\_dht\_node}{init}{/1} is finished the supervisor module
starts all the defined processes by calling the functions that were
defined in the returned list.

For a join of a new node, we are only interested in the starting of
the \code{SupDHTNodeCore_AND} process here. At that point in time, all
other defined processes are already started and running.

\section{Starting the sup\_dht\_node\_core supervisor with a peer and some paxos processes}

Like any other supervisor the \erlmodule{sup\_dht\_node\_core} supervisor
calls its \erlfun[]{sup\_dht\_node\_core}{init}{/1} function:

\codesnippet{sup_dht_node_core.erl}{sup_dht_node_core:init}{../src/sup_dht_node_core.erl}

It defines five processes, that have to be observed using a
\code{one_for_all}-supervisor, which means, that if one fails, all have to
be restarted. The \erlmodule{dht\_node} module implements the main
component of a full \scalaris{} node which glues together all the other
processes. Its
\erlfun[]{dht\_node}{start\_link}{/2} function will get
the following parameters: (a) the processes' group that is used with the \erlmodule{pid\_groups}
module and (b) a list of options for
the \erlmodule{dht\_node}. The process group name was calculated
a bit earlier in the code. \emph{Exercise: Try to find where.}


\codesnippet{dht_node.erl}{dht_node:start_link}{../src/dht_node.erl}

Like many other modules, the \erlmodule{dht\_node} module implements the
\code{gen_component}
behaviour. This behaviour was developed by us to enable us to write code which is
similar in syntax and semantics to the examples in~\cite{rachid-book}.
Similar to the \code{supervisor} behaviour, a module implementing this
behaviour has to
provide an \code{init/1} function, but here it is used to initialize the
state of the component. This function is described in the next
section.

Note: \code{?MODULE} is a predefined Erlang macro, which expands to the
module name, the code belongs to (here: \code{dht_node}).

\section{\texorpdfstring{Initializing a \code{dht_node}-process}
             {Initializing a dht\_node-process}}

\codesnippet{dht_node.erl}{dht_node:start}{../src/dht_node.erl}

The \code{gen_component} behaviour registers the \code{dht_node} in the
process dictionary. Formerly, the process had to do this itself, but
we moved this code into the behaviour. If an ID was given to
\erlfun{dht\_node}{init}{/1} function as a \code{\{\{dht_node, id\}, KEY\}}
tuple, the given Id will be used. Otherwise a random key is generated.
Depending on whether the node is the first inside a VM marked as first or not,
the according function in \erlmodule{dht\_node\_join} is called.
Also the pid of the node's supervisor is kept for future reference.

\section{Actually joining the ring}

After retrieving its identifier, the node starts the join protocol
which processes the appropriate messages calling
\erlfun{dht\_node\_join}{process\_join\_state}{Message, State}. On the
existing node, join messages will be processed by
\erlfun{dht\_node\_join}{process\_join\_msg}{Message, State}.

\subsection{A single node joining an empty ring}

\codesnippet{dht_node_join.erl}{dht_node_join:join_as_first}{../src/dht_node_join.erl}

If the ring is empty, the joining node will be the only node in the ring
and will thus be responsible for the whole key space.
It will trigger all known nodes to initialize the comm layer and then finish
the join.
\erlfun{dht\_node\_join}{finish\_join}{/5}
just creates a new state for a \scalaris{} node consisting of the given
parameters (the node as itself, its predecessor and successor,
an empty database and the queued messages that arrived during the join).
It then activates all dependent processes and creates a routing table from
this information.

The \erlfun{dht\_node\_state}{state}{} type is defined in

\codesnippet{dht_node_state.erl}{dht_node_state:state}{../src/dht_node_state.erl}

\subsection{A single node joining an existing (non-empty) ring}

If a node joins an existing ring, its join protocol will step through
the following four phases:
\begin{itemize}
  \item \makebox[4em][l]{\textbf{phase2}}
    finding nodes to contact with the help of the configured \code{known_hosts}
  \item \makebox[4em][l]{\textbf{phase2b}}
    getting the number of Ids to sample (may be skipped)
  \item \makebox[4em][l]{\textbf{phase3}}
    lookup nodes responsible for all sampled Ids
  \item \makebox[4em][l]{\textbf{phase4}}
    joining a selected node and setting up item movements
\end{itemize}

The following figure shows a (non-exhaustive) overview of the transitions
between the phases in the normal case. We will go through these step by step
and discuss what happens if errors occur.

\medskip
{\centering\input{dht_node_join_phases2.tikz}}

At first all nodes set in the \code{known_hosts} configuration parameter are
contacted. Their responses are then handled in phase~2. In order to separate
the join state from the ordinary \code{dht_node} state, the
\code{gen_component} is instructed to use the \erlfun{dht\_node}{on\_join}{/2}
message handler which delegates every message to
\erlfun{dht\_node\_join}{process\_join\_state}{/2}.

\codesnippet{dht_node_join.erl}{dht_node_join:join_as_other}{../src/dht_node_join.erl}

\subsubsection{Phase 2 and 2b}

Phase~2 collects all \erlmodule{dht\_node} processes inside the contacted
VMs. It therefore mainly processes \code{get_dht_nodes_response} messages and
integrates all received nodes into the list of available connections.
The next step depends on whether the \code{\{skip_psv_lb\}} option for
skipping any passive load balancing algorithm has been
given to the \erlmodule{dht\_node} or not. If it is present, the node will
only use the ID that has been initially passed to
\erlfun{dht\_node\_join}{join\_as\_other}{/3}, issue a lookup for the
responsible
node and move to phase~3. Otherwise, the passive load balancing's
\erlfun{lb\_psv\_*}{get\_number\_of\_samples}{/1} method will be called
asking for the number of IDs to sample. Its answer will be processed in
phase~2b.

\code{get_dht_nodes_response} messages arriving in phase~2b or later will be
processed anyway and received \erlmodule{dht\_node}
processes will be integrated into the connections. These phases'
operations will not be interrupted and nothing else is changed though.

\codesnippet{dht_node_join.erl}{dht_node_join:join_other_p2}{../src/dht_node_join.erl}

Phase~2b will handle \code{get_number_of_samples} messages from the passive
load balance algorithm. Once received, new (unique) IDs will be sampled
randomly so that the total number of join candidates (selected IDs together
with fully processed candidates from further phases) is at least as high as
the given number of samples. Afterwards, lookups will be created for all
previous IDs as well as the new ones and the node will move to phase~3.

\codesnippet{dht_node_join.erl}{dht_node_join:join_other_p2b}{../src/dht_node_join.erl}

Lookups will make \scalaris{} find the node currently responsible for a given ID
and send a request to simulate a join to this node, i.e. a
\code{get_candidate} message. Note that during such an operation, the joining
node would become the existing node's predecessor. The simulation will be
delegated to the passive load balance algorithm the joining node requested, as
set by the \code{join_lb_psv} configuration parameter. 

\codesnippet{dht_node_join.erl}{dht_node_join:get_candidate}{../src/dht_node_join.erl}

\subsubsection{Phase 3}

The result of the simulation will be send in a \code{get_candidate_response}
message and will be processed in phase~3 of the joining node. It will be
integrated into the list of processed candidates. If there are no more IDs
left to process, the best among them will be contacted. Otherwise further
\code{get_candidate_response} messages will be awaited.
Such messages will also be processed in the other phases where the candidate
will be simply added to the list.

\codesnippet{dht_node_join.erl}{dht_node_join:join_other_p3}{../src/dht_node_join.erl}

If \erlfun{dht\_node\_join}{contact\_best\_candidate}{/1} is called and
candidates are available (there should be at this stage!), it will sort the
candidates by using the passive load balance algorithm, send a 
\code{join_request} message and continue with phase~4. 

\erlfunindex{dht\_node\_join}{contact\_best\_candidate}
\codesnippet{dht_node_join.erl}{dht_node_join:contact_best_candidate}{../src/dht_node_join.erl}
\erlfunindex{dht\_node\_join}{send\_join\_request}
\codesnippet{dht_node_join.erl}{dht_node_join:send_join_request}{../src/dht_node_join.erl}

The \code{join_request} message will be received by the existing node which
will set up a slide operation with the new node. If it is not responsible for
the key (anymore), it will deny the request and reply with a
\code{\{join, join_response, not_responsible, Node\}} message.

\codesnippet{dht_node_join.erl}{dht_node_join:join_request1}{../src/dht_node_join.erl}

If it is responsible for the ID and is not participating in a slide with its
current predecessor, it will set up a slide with the joining node:

\erlfunindex{dht\_node\_join}{send\_join\_response}
\codesnippet{dht_node_join.erl}{dht_node_join:join_request2}{../src/dht_node_join.erl}

\subsubsection{Phase 4}

The joining node will receive the \code{join_response} message in phase~4 of
the join protocol. If everything is ok, it will
notify its ring maintenance process that it enters the ring, start all required
processes and join the slide operation set up by the existing node in order to
receive some of its data.

If the join candidate's node is not responsible for the candidate's ID anymore
or the candidate's ID already exists, the next candidate is contacted until
no further candidates are available and the join protocol starts over using
\erlfun{dht\_node\_join}{start\_over}{/1}.

Note that the \code{join_response} message will actually be processed in
any phase. Therefore, if messages arrive late, the join can be processed
immediately and the rest of the join protocol does not need to be executed
again.

\codesnippet{dht_node_join.erl}{dht_node_join:join_other_p4}{../src/dht_node_join.erl}
\erlfunindex{dht\_node\_join}{finish\_join}
\erlfunindex{dht\_node\_join}{finish\_join\_and\_slide}
\codesnippet{dht_node_join.erl}{dht_node_join:finish_join}{../src/dht_node_join.erl}

The macro \code{?RT} maps to the configured routing algorithm. It is defined
in \code{include/scalaris.hrl}. For further details on the routing see
Chapter~\sieheref{chapter.routing}.

\subsubsection{Timeouts and other errors}

The following table summarizes the timeout messages send during the join
protocol on the joining node. It shows in which of the phases each of the
messages is processed and describes (in short) what actions are taken.
All of these messages are influenced by their respective config parameters,
e.g. \code{join_timeout} parameter in the config files defines an overall
timeout for the whole join operation. If it takes longer than
\code{join_timeout} ms, a \code{\{join, timeout\}} will be send and processed
as given in this table.

\medskip
{\small
\begin{tabular}{lP{2.4cm}P{2.75cm}P{2.35cm}P{3.25cm}M{1.70cm}}
  \toprule
  & \code{known_hosts}\carriagereturn\newline\code{_timeout}
  & \code{get_number_of}\carriagereturn\newline\code{_samples}\carriagereturn\newline\code{_timeout}
  & \code{lookup}\carriagereturn\newline\code{ _timeout}
  & \code{join_request}\carriagereturn\newline\code{_timeout}
  & \code{timeout} \tn
  \midrule
  %
  \bfseries phase2
  & get known nodes from configured VMs
  & ignore
  & ignore
  & ignore
  & \multirow{21}{1.70cm}
        {re-start join\newline$\rightarrow$ phase~2 or 2b} \tn
  \cmidrule(r){1-5}
  %
  \bfseries phase2b
  & ignore
  & remove contact node, re-start join\newline
    $\rightarrow$ phase~2 or 2b
  & ignore
  & ignore
  & \tn
  \cmidrule(r){1-5}
  %
  \bfseries phase3
  & ignore
  & ignore
  & remove contact node, lookup remaining IDs\newline
    $\rightarrow$ phase~2 or 3 
  & ignore
  & \tn
  \cmidrule(r){1-5}
  %
  \bfseries phase3b
  & ignore
  & ignore
  & ignore
  & ignore
  & \tn
  \cmidrule(r){1-5}
  %
  \bfseries phase4
  & ignore
  & ignore
  & ignore
  & timeouts $< 3$?\footnote{set by the \code{join_request_timeouts} config parameter}\newline
    \mbox{}~$\rightarrow$ contact candidate\newline
    otherwise:\newline
    \mbox{}~remove candidate\newline
    \mbox{}~no candidates left?\newline
    \mbox{}~~$\rightarrow$ phase~2 or 2b\newline
    \mbox{}~otherwise:\newline
    \mbox{}~~$\rightarrow$ contact next one\newline
    \mbox{}~~$\rightarrow$ phase~3b or 4
  & \tn
  \bottomrule
\end{tabular}
}
\medskip

On the existing node, there is only one timeout message which is part of the
join protocol: the \code{join_response_timeout}. It will be send when a slide
operation is set up and if the timeout hits before the next message exchange,
it will increase the slide operation's number of timeouts. The slide will be
aborted if at least \code{join_response_timeouts} timeouts have been received.
This parameter is set in the config file.

\subsubsection{Misc. (all phases)}

Note that join-related messages arriving in other phases than those handling
them will be ignored. Any other messages during a \code{dht_node}'s join will
be queued and re-send when the join is complete.

\chapter{Directory Structure of the Source Code}

The directory tree of \scalaris{} is structured as follows:

\vspace*{1em}
\begin{tabular}{|r|p{11.5cm}|}
 \hline
 \code{bin} & contains shell scripts needed to work with \scalaris{} (e.g.\ start the management server, start a node, \dots)\\
 \code{contrib} & necessary third party packages (yaws and log4erl) \\
 \code{doc} & generated Erlang documentation \\
 \code{docroot} & root directory of the node's webserver \\
 \code{ebin} & the compiled Erlang code (beam files)\\
 \code{java-api} & a Java API to \scalaris{} \\
 \code{log} & log files \\
 \code{src} & contains the \scalaris{} source code\\
 \code{test} & unit tests for \scalaris{} \\
 \code{user-dev-guide} & contains the sources for this document\\
 \hline
\end{tabular}

%\chapter{System Components}



%\chapter{Processes}

%\chapter{Troubleshooting}

%\section{ApplicationMonitor appmon:start()}

\chapter{Java API}

For the Java API documentation, we refer the reader to the documentation
generated by javadoc or doxygen. The following commands create the
documentation:

\begin{lstlisting}[language=sh]
%> cd java-api
%> ant doc
%> doxygen
\end{lstlisting}

The documentation can then be found in \code{java-api/doc/index.html}
(javadoc) and\\ \code{java-api/doc-doxygen/html/index.html} (doxygen).

The API is divided into four classes:

\begin{itemize}
\item \code{de.zib.scalaris.Transaction} for (multiple) operations inside a
       transaction
\item \code{de.zib.scalaris.TransactionSingleOp} for single transactional
       operations
\item \code{de.zib.scalaris.ReplicatedDHT} for non-transactional (inconsistent)
       access to the replicated DHT items, e.g. deleting items
\item \code{de.zib.scalaris.PubSub} for topic-based publish/subscribe
       operations
\end{itemize}

\bibliographystyle{plainnat}

\begin{thebibliography}{9}

\bibitem{erlang-book}
Joe Armstrong.
\newblock \emph{Programming Erlang: Software for a Concurrent World.}
\newblock Pragmatic Programmers, ISBN: 978-1-9343560-0-5, July 2007

\bibitem{vivaldi}
Frank Dabek, Russ Cox, Frans Kaahoek, Robert Morris.
\newblock \emph{Vivaldi: A Decentralized Network Coordinate System.}
\newblock ACM SIGCOMM 2004.

\bibitem{rachid-book}
Rachid Guerraoui and Luis Rodrigues.
\newblock \emph{Introduction to Reliable Distributed Programming.}
\newblock Springer-Verlag, 2006.

\bibitem{chord-sigcomm}
Ion Stoica, Robert Morris, David Karger, M. Frans Kaashoek and Hari Balakrishnan.
\newblock \emph{Chord: A Scalable Peer-to-peer Lookup Service for Internet Applications.}
\newblock ACM SIGCOMM 2001, San Deigo, CA, August 2001, pp. 149-160.
\newblock \href{http://pdos.csail.mit.edu/papers/chord:sigcomm01/chord_sigcomm.pdf}{http://pdos.csail.mit.edu/papers/chord:sigcomm01/chord\_sigcomm.pdf}

\bibitem{t-man}
M{\'a}rk Jelasity, Alberto Montresor, Ozalp Babaoglu.
\newblock \emph{T-Man: Gossip-based fast overlay topology construction.}
\newblock Computer Networks (CN) 53(13):2321-2339, 2009.

\bibitem{enhanced-paxos}
F. Schintke, A. Reinefeld, S. Haridi, T. Sch{\"u}tt.
\newblock \emph{Enhanced Paxos Commit for Transactions on DHTs.}
\newblock 10th IEEE/ACM Int. Conf. on Cluster, Cloud and Grid Computing, pp. 448-454,
May 2010.

\bibitem{cyclon}
Spyros Voulgaris, Daniela Gavidia, Maarten van Steen.
\newblock \emph{CYCLON: Inexpensive Membership Management for Unstructured P2P Overlays.}
\newblock J. Network Syst. Manage. 13(2): 2005.

\bibitem{gossip}
M{\'a}rk Jelasity, Alberto Montresor, Ozalp Babaoglu.
\newblock \emph{Gossip-based aggregation in large dynamic networks.}
\newblock ACM Trans. Comput. Syst. 23(3), 219-252 (2005).

\end{thebibliography}

\printindex

\end{document}
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 

